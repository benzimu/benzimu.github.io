[{"categories":["etcd"],"content":"etcd leader 频繁切换 ","date":"2024-08-03","objectID":"/posts/leader_frequently_change/:1:0","tags":["etcd","leader"],"title":"etcd leader 频繁切换","uri":"/posts/leader_frequently_change/"},{"categories":["etcd"],"content":"背景说明 管控服务依赖 etcd 进行选主，项目测试开发阶段均正常。项目交互后在客户环境部署，etcd 偶发性出现 leader 切换，但频率很低，几天出现一次。项目用量上升后开发频繁出现 leader 切换。 etcd 集群节点分别部署在三台虚拟机中，磁盘使用 Ceph 共享存储。每台虚拟机 IOPS 已设置为最大 1000。 ","date":"2024-08-03","objectID":"/posts/leader_frequently_change/:1:1","tags":["etcd","leader"],"title":"etcd leader 频繁切换","uri":"/posts/leader_frequently_change/"},{"categories":["etcd"],"content":"问题排查 管控服务（etcd 客户端）日志 异常日志1 rpc error: code = Unavailable desc = etcdserver: leader changed 异常日志2 rpc error: code = Unknown desc = context deadline exceeded 从客户端服务日志可知，由于 etcd leader 切换，导致服务异常。 etcd 日志 异常日志1 waiting for ReadIndex response took too long 异常日志2 {\"level\":\"warn\",\"msg\":\"slow fdatasync\",\"took\":\"2.14025047s\",\"expected-duration\":\"1s\"} 异常日志3 raft.node: 255a2e4092d561fb changed leader from 255a2e4092d561fb to 1de1eaa8fb268f49 at term 3112 从 etcd 日志可以确认，由于 slow fdatasync，导致 etcd leader 切换。 查看虚拟机监控 etcd leader 异常切换期间，虚拟机 CPU、内存使用率均在正常范围内。并且 IOPS、吞吐量、await延时等监控也均在有效范围内，所以暂时无法确定是否与 Ceph 共享存储性能问题。 但磁盘监控中存在一个异常指标，disk.io.msec_write（所有写入操作所花费的总时间）： leader_frequently_change_03 从监控看该指标最高达到60s，且每次峰值均能与 etcd slow fdatasync 日志时间吻合，可确认 etcd 频繁 leader changed 与磁盘性能有关。 ","date":"2024-08-03","objectID":"/posts/leader_frequently_change/:1:2","tags":["etcd","leader"],"title":"etcd leader 频繁切换","uri":"/posts/leader_frequently_change/"},{"categories":["etcd"],"content":"问题确认 etcd leader 每 100ms 向 follower 发送心跳，维持 leader 地位。由于磁盘性能不足，导致 etcd fdatasync 较慢，超过 etcd follower 选举超时时间（election timeout，默认 1000ms，约10个心跳间隔），从而 etcd 其他 follower 节点开始新一轮 leader 选举，这就是 etcd 服务日志中看到的频繁 Leader changed 的原因。 从 disk.io.msec_write 监控图可以看出，峰值之间间隔约 2 小时，即每两小时出现一次 msec_write 升高，且 etcd 服务日志中出现一次 leader changed。查看服务部署拓扑图，发现三台 etcd 节点上分别都部署了 Prometheus 服务。 了解 Prometheus 存储原理的应该知道，Prometheus 新写入的数据保存在内存 block 中，2 小时后将 block 数据写入磁盘。同时，为了防止程序崩溃导致数据丢失，实现了 WAL（write-ahead-log）机制，服务启动时会以写入日志（WAL）的方式来实现重播，从而恢复数据。 为验证猜想，将虚拟机上 Prometheus 服务停掉后，etcd 服务运行正常，没有再出现 leader changed。 项目上线早期，业务量小，Prometheus 监控数据也比较小，所以偶发性出现 etcd leader changed。业务量上涨后，Prometheus 监控数据上涨，导致每 2 小时数据写入磁盘时，占用大量磁盘IO性能，影响 etcd 性能。 ","date":"2024-08-03","objectID":"/posts/leader_frequently_change/:1:3","tags":["etcd","leader"],"title":"etcd leader 频繁切换","uri":"/posts/leader_frequently_change/"},{"categories":["etcd"],"content":"解决方案 知道了问题，解决方案就比较明了了，直接将 etcd 单独部署，不与其他业务耦合，减少相互之间的影响。 ","date":"2024-08-03","objectID":"/posts/leader_frequently_change/:1:4","tags":["etcd","leader"],"title":"etcd leader 频繁切换","uri":"/posts/leader_frequently_change/"},{"categories":["etcd"],"content":"知识点扫盲 etcd fdatasync fdatasync 与 fsync 都是系统调用，用于将块缓冲区数据写入磁盘。fsync 会等待写磁盘操作结束，然后同步返回，操作完成前会阻塞。可用于数据库这样的应用程序，这种应用程序需要确保将修改过的块立即写到磁盘上。fsync 会将文件的所有元数据（如文件的大小、权限、访问时间等）和文件内容一起同步到磁盘。而 fdatasync 仅同步文件内容及其与文件大小相关的元数据，而不处理不必要的元数据（如访问时间等）。因此，fdatasync 的开销通常比 fsync 更小，性能更好。 每当客户端添加或更新键值对数据时，etcd 会向 WAL 文件添加一条入库记录条目。再进一步处理之前，etcd 必须 100% 确保 WAL 条目已经被持久化。 要在 Linux 实现这一点，仅使用write系统调用是不够的，因为对物理存储的写入操作可能会发生延迟。比如，Linux 可能会将写入的 WAL 条目在内核内存缓存中保留一段时间（例如，页缓存）。如果要确保数据被写入持久化存储，你必须在 write 系统调用之后调用 fdatasync 系统调用。 ectd 实践推荐 由于 etcd 将数据写入磁盘并将提案保留在磁盘上，因此其性能取决于磁盘性能。尽管 etcd 不是特别 I/O 密集型的，但它需要低延迟块设备才能获得最佳性能和稳定性。由于 etcd 的共识协议依赖于将元数据持久存储到日志 (WAL)，因此 etcd 对磁盘写入延迟很敏感。慢速磁盘和其他进程的磁盘活动可能会导致较长的 fsync 延迟。 这些延迟可能会导致 etcd 错过心跳，无法及时将新提案提交到磁盘，并最终导致请求超时和暂时失去领导者。同样导致 etcd 心跳时间超过选举超时时间，从而破坏集群的领导者选举。基于这些原因，应避免将对 I/O 敏感或密集且共享相同底层 I/O 基础架构的控制平面节点上的其他工作负载共置在一起。 就延迟而言，在块设备上运行 etcd，该设备可以连续写入至少 50 IOPS，长度为 8000 字节。也就是说，延迟为 10 毫秒，请记住使用 fdatasync 同步 WAL 中的每个写入。对于负载较重的集群，建议连续 500 IOPS，长度为 8000 字节（2 毫秒）。要测量这些数字，可以使用基准测试工具，例如 fio。 为了实现这样的性能，应在具有低延迟和高吞吐量的 SSD 或 NVMe 磁盘支持的机器上运行 etcd。同样可以将 etcd 从共享磁盘移动到单独的磁盘，以防止或解决性能问题。 etcd 性能影响因素 etcd 提供稳定、持续的高性能。两个因素决定了性能：延迟和吞吐量。延迟是完成操作所需的时间。吞吐量是某个时间段内完成的总操作数。通常，当 etcd 接受并发客户端请求时，平均延迟会随着总体吞吐量的增加而增加。 etcd 使用 Raft 共识算法在成员之间复制请求并达成一致。共识性能（尤其是提交延迟）受到两个物理约束的限制：网络 IO 延迟和磁盘 IO 延迟。完成 etcd 请求的最短时间是成员之间的网络往返时间 (RTT) 加上 fdatasync 将数据提交到永久存储所需的时间。数据中心内的 RTT 可能长达几百微秒。美国境内的典型 RTT 约为 50 毫秒，而各大洲之间的 RTT 可能最慢为 400 毫秒。旋转磁盘的典型 fdatasync 延迟约为 10 毫秒。对于 SSD，延迟通常低于 1 毫秒。为了提高吞吐量，etcd 将多个请求批量处理并将它们提交给 Raft。这种批处理策略使 etcd 即使在负载很重的情况下也能实现高吞吐量。 还有其他子系统会影响 etcd 的整体性能。每个序列化的 etcd 请求都必须通过 etcd 的 boltdb 支持的 MVCC 存储引擎，这通常需要几十微秒才能完成。etcd 会定期增量快照其最近应用的请求，并将它们与之前的磁盘快照合并。此过程可能会导致延迟峰值。虽然这在 SSD 上通常不是问题，但它可能会使 HDD 上观察到的延迟加倍。同样，压缩也会影响 etcd 的性能。 ","date":"2024-08-03","objectID":"/posts/leader_frequently_change/:1:5","tags":["etcd","leader"],"title":"etcd leader 频繁切换","uri":"/posts/leader_frequently_change/"},{"categories":["Kafka"],"content":"Topic 分区 ISR 混乱，为 AR 超集 ","date":"2024-02-25","objectID":"/posts/partition_isr_confusion/:1:0","tags":["Kafka","ISR","AR"],"title":"Topic partition ISR 混乱，为 AR 超集","uri":"/posts/partition_isr_confusion/"},{"categories":["Kafka"],"content":"背景说明 有一个四节点的 Kafka 集群，Broker id 分别为 1、2、3、4。修改集群配置后滚动重启各节点，重启完成后，生产者写入消息失败。 ","date":"2024-02-25","objectID":"/posts/partition_isr_confusion/:1:1","tags":["Kafka","ISR","AR"],"title":"Topic partition ISR 混乱，为 AR 超集","uri":"/posts/partition_isr_confusion/"},{"categories":["Kafka"],"content":"问题排查 查看 Broker 日志 Uncaught exception in scheduled task 'isr-expiration' org.apache.kafka.common.errors.NotLeaderOrFollowerException: Replica with id 2 is not available on broker 1 partition_isr_confusion_01 该错误信息与官方 issue KAFKA-9672 类似，均是由于 Broker 节点异常下线导致。 查看 Topic 详情 此时出现部分 Topic 分区 ISR 混乱情况，即 ISR 副本集与 AR 副本集不一致，现象显示 ISR 为 AR 超集，与实际情况相悖。如图所示： partition_isr_confusion_02 ","date":"2024-02-25","objectID":"/posts/partition_isr_confusion/:1:2","tags":["Kafka","ISR","AR"],"title":"Topic partition ISR 混乱，为 AR 超集","uri":"/posts/partition_isr_confusion/"},{"categories":["Kafka"],"content":"解决方案 直接修改 ZooKeeper 中 Topic 分区 ISR 信息，如图： partition_isr_confusion_03 # 获取指定 topic 状态信息 get /brokers/topics/testtopic/partitions/0/state # 手动修改 topic isr 信息 set /brokers/topics/testtopic/partitions/0/state {\"controller_epoch\":1,\"leader\":3,\"version\":1,\"leader_epoch\":0,\"isr\":[3,1,2]} 修改完成后，需删除 zk 上 /controller 节点，重新选举 Controller，用于更新 Kafka 集群控制器内存数据 delete /controller ","date":"2024-02-25","objectID":"/posts/partition_isr_confusion/:1:3","tags":["Kafka","ISR","AR"],"title":"Topic partition ISR 混乱，为 AR 超集","uri":"/posts/partition_isr_confusion/"},{"categories":["Kafka"],"content":"使用 Tcpdump + Wireshark 抓包获取 Kafka 指定 Topic 生产者连接 IP ","date":"2023-07-15","objectID":"/posts/tcpdump_producer_ip/:1:0","tags":["Kafka","Tcpdump","Wireshark"],"title":"Tcpdump + Wireshark 分析 Kafka 数据包","uri":"/posts/tcpdump_producer_ip/"},{"categories":["Kafka"],"content":"背景 有一个 Kafka 集群，公司各业务共同使用。但由于公司架构调整，人员变动，导致部分 Topic 使用方信息缺失，需要进行梳理。 开源 Kafka 不支持查询指定 Topic 对应的生产者连接 IP，但能通过 kafka-consumer-groups.sh 脚本获取消费者连接 IP。于是需要通过抓包分析数据，获取指定 Topic 生产者连接 IP。 ","date":"2023-07-15","objectID":"/posts/tcpdump_producer_ip/:1:1","tags":["Kafka","Tcpdump","Wireshark"],"title":"Tcpdump + Wireshark 分析 Kafka 数据包","uri":"/posts/tcpdump_producer_ip/"},{"categories":["Kafka"],"content":"环境准备 Kafka 集群节点上安装 Tcpdump 本机安装 Wireshark ","date":"2023-07-15","objectID":"/posts/tcpdump_producer_ip/:1:2","tags":["Kafka","Tcpdump","Wireshark"],"title":"Tcpdump + Wireshark 分析 Kafka 数据包","uri":"/posts/tcpdump_producer_ip/"},{"categories":["Kafka"],"content":"Tcpdump 抓包 确认 Topic 副本分区分布，如无特殊使用场景（例如指定分区生产等），则可任选一个分区节点抓包 登录 Kafka 集群节点，执行抓包命令 # port 9092: Kafka 进程端口 # -i any: 网卡名称，any 表示所有网卡 # -s 0: 抓取数据包时默认抓取长度为68字节，加上 -s 0 后可以抓到完整的数据包 # -w ukafka-rzvtagn5-kafka1.%Y%m%d_%H%M%S.pcap: 将包写入文件中，使用时间格式命名文件 # -G 60: 每分钟切割一次文件，避免单个文件太大 # -Z root: 指定运行 tcpdump 进程的用户权限为 root tcpdump port 9092 -i any -s 0 -w test.%Y%m%d_%H%M%S.pcap -G 60 -Z root 将数据包文件传输到本地 ","date":"2023-07-15","objectID":"/posts/tcpdump_producer_ip/:1:3","tags":["Kafka","Tcpdump","Wireshark"],"title":"Tcpdump + Wireshark 分析 Kafka 数据包","uri":"/posts/tcpdump_producer_ip/"},{"categories":["Kafka"],"content":"Wireshark 分析 前提 确认 Wireshark 是否支持 Kafka 协议，如不支持，建议升级 Wireshark 版本 Wireshark 支持 Kafka 协议说明：https://www.wireshark.org/docs/dfref/k/kafka.html tcpdump_producer_ip_01 了解 Kafka 协议标准：https://kafka.apache.org/0101/protocol.html 使用 Wireshark 加载数据包文件 tcpdump_producer_ip_02 填写过滤表达式，获取指定数据包 # kafka.topic_name == lbtopic 过滤 Topic 名称为 lbtopic # kafka.api_key == 0 过滤 Kafka API key 为 0（0为生产） kafka.topic_name == lbtopic and kafka.api_key == 0 kafka.api_key 参考 Kafka 协议标准 Api Keys 说明 tcpdump_producer_ip_03 从中可以得知如下信息： 生产端IP 网络协议 请求类型 客户端端口 tcpdump_producer_ip_04 同时，可以查看 Kafka 数据包具体信息。 统计客户端所有 IP 地址 打开统计页面：统计 -\u003e IPv4 Statistics -\u003e All Addresses tcpdump_producer_ip_05 填写过滤规则，获取客户端IP（注意去掉本机IP）： tcpdump_producer_ip_06 最终可确认生产者IP地址为： 10.9.78.181 ","date":"2023-07-15","objectID":"/posts/tcpdump_producer_ip/:1:4","tags":["Kafka","Tcpdump","Wireshark"],"title":"Tcpdump + Wireshark 分析 Kafka 数据包","uri":"/posts/tcpdump_producer_ip/"},{"categories":["Kubernetes"],"content":"使用 kubeadm 在 CentOS 8.3 上安装 k8s 集群 ","date":"2023-07-02","objectID":"/posts/k8s_deploy/:1:0","tags":["Kubernetes","Kubeadm"],"title":"使用 kubeadm 安装 k8s 集群","uri":"/posts/k8s_deploy/"},{"categories":["Kubernetes"],"content":"清单 服务器 IP 主机名 配置 节点类别 10.9.56.239 master1-10-9-56-239 2核/4G/200G磁盘 master 10.9.158.227 master2-10-9-158-227 2核/4G/200G磁盘 master 10.9.153.152 master3-10-9-153-152 2核/4G/200G磁盘 master 10.9.167.63 node-10-9-167-63 2核/4G/200G磁盘 worker 10.9.32.181 node-10-9-32-181 2核/4G/200G磁盘 worker 10.9.63.155 node-10-9-63-155 2核/4G/200G磁盘 worker 软件版本 软件 版本 系统 CentOS 8.3 Kubernetes v1.27.3 Docker 20.10.21 Keepalived 2.0.17 Haproxy 2.1.4 ","date":"2023-07-02","objectID":"/posts/k8s_deploy/:1:1","tags":["Kubernetes","Kubeadm"],"title":"使用 kubeadm 安装 k8s 集群","uri":"/posts/k8s_deploy/"},{"categories":["Kubernetes"],"content":"基础环境配置 如无特殊说明，所有节点均需要执行操作 确保每个节点上 MAC 地址和 product_uuid 的唯一性 # 检测 mac 地址 ip link ifconfig -a # 检查 product_uuid cat /sys/class/dmi/id/product_uuid 修改服务器 hostname # 在 10.9.56.239 上执行 hostnamectl set-hostname master1-10-9-56-239 # 在 10.9.158.227 上执行 hostnamectl set-hostname master2-10-9-158-227 # 在 10.9.153.152 上执行 hostnamectl set-hostname master3-10-9-153-152 退出，重新登录，可以看到主机名生效。 配置服务器 hosts 映射关系对 10.9.56.239 cluster-endpoint 与 kubeadm init 时传递参数 --control-plane-endpoint=cluster-endpoint 关联，配置高可用集群时，该参数必须指定。 高可用集群部署成功后，可将该映射 IP 地址改为负载均衡地址。 该映射 IP 为第一个部署的 master 节点地址。 cat \u003e\u003e /etc/hosts\u003c\u003cEOF 10.9.56.239 master1-10-9-56-239 10.9.158.227 master2-10-9-158-227 10.9.153.152 master3-10-9-153-152 10.9.56.239 cluster-endpoint EOF 配置 ssh 互信 # 生成 ssh 公私钥 ssh-keygen # 配置 authorized_keys（各节点交叉执行） ssh-copy-id -i ~/.ssh/id_rsa.pub root@10.9.56.239 ssh-copy-id -i ~/.ssh/id_rsa.pub root@10.9.158.227 ssh-copy-id -i ~/.ssh/id_rsa.pub root@10.9.153.152 关闭防火墙 systemctl stop firewalld systemctl disable firewalld 关闭 swap，注释 swap 分区 # 临时有效 swapoff -a # 永久生效，修改 /etc/fstab，删除如下行 /dev/mapper/cl-swap swap swap defaults 0 0 禁用 SELinux setenforce 0 sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config 配置内核参数，转发 IPv4 并让 iptables 看到桥接流量 参考：k8s 容器运行时转发 IPv4 并让 iptables 看到桥接流量 cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # 设置所需的 sysctl 参数，参数在重新启动后保持不变 cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF # 应用 sysctl 参数而不重新启动 sudo sysctl --system 添加阿里源 # 配置yum源 cd /etc/yum.repos.d ; mkdir bak; mv CentOS-Base.repo bak/; mv CentOS-Linux-* bak/ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-8.repo 安装常用包 yum install -y vim bash-completion net-tools gcc 设置时间同步 yum install -y chrony # 同步的时间服务器修改为国家授时中心 NTP 服务器 sed -i 's/^pool 2.centos.pool.ntp.org iburst$/pool ntp.ntsc.ac.cn iburst/' /etc/chrony.conf systemctl start chronyd systemctl enable chronyd # 查看同步的时间服务器 chronyc sources 安装 docker yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum -y install docker-ce 安装 docker 如果报错： [root@master1-10-9-56-239 ~]# yum install docker-ce Repository cr is listed more than once in the configuration Repository extras is listed more than once in the configuration Repository extras-source is listed more than once in the configuration Repository fasttrack is listed more than once in the configuration Docker CE Stable - x86_64 66 kB/s | 29 kB 00:00 Error: Problem: problem with installed package buildah-1.22.3-2.module_el8.5.0+911+f19012f9.x86_64 - package buildah-1.22.3-2.module_el8.5.0+911+f19012f9.x86_64 requires runc \u003e= 1.0.0-26, but none of the providers can be installed - package containerd.io-1.4.3-3.1.el8.x86_64 conflicts with runc provided by runc-1.0.2-1.module_el8.5.0+911+f19012f9.x86_64 ... - cannot install the best candidate for the job - package runc-1.0.0-56.rc5.dev.git2abd837.module_el8.3.0+569+1bada2e4.x86_64 is filtered out by modular filtering - package runc-1.0.0-66.rc10.module_el8.5.0+1004+c00a74f5.x86_64 is filtered out by modular filtering - package runc-1.0.0-72.rc92.module_el8.5.0+1006+8d0e68a2.x86_64 is filtered out by modular filtering (try to add '--allowerasing' to command line to replace conflicting packages or '--skip-broken' to skip uninstallable packages or '--nobest' to use not only best candidate packages) 解决办法 # 删除 podman、buildah yum remove -y podman buildah containers-common # 再重新执行 docker 安装命令 yum -y install docker-ce 启动 docker systemctl start docker systemctl enable docker 添加阿里云 docker 仓库加速器： cat \u003e/etc/docker/daemon.json\u003c\u003cEOF { \"registry-mirrors\": [\"https://fl791z1h.mirror.aliyuncs.com\"] } EOF systemctl reload docker systemctl status docker containerd 安装 docker 后，默认会自动安装容器运行时 containerd。kubernetes 也支持使用 containerd 作为容器运行时。 重载沙箱（","date":"2023-07-02","objectID":"/posts/k8s_deploy/:1:2","tags":["Kubernetes","Kubeadm"],"title":"使用 kubeadm 安装 k8s 集群","uri":"/posts/k8s_deploy/"},{"categories":["Kubernetes"],"content":"使用 kubeadm 创建集群 参考：使用 kubeadm 创建集群 部署 master1 节点 master1 上执行 kubeadm init # –-apiserver-advertise-address 用于为控制平面节点的 API server 设置广播地址。 # –-image-repository 指定从什么位置来拉取镜像。 # --control-plane-endpoint 用于为所有控制平面节点设置共享端点。如果不设置，则无法将单个控制平面 kubeadm 集群升级成高可用。 # --upload-certs 指定将在所有控制平面实例之间的共享证书上传到集群。后面 join 其他 master 节点时，可以使用该证书。 # –-kubernetes-version 指定 k8s 版本号。 # –-pod-network-cidr 指定 Pod 网络的范围。不同 CNI 默认网段也不一样，Calico 默认为 192.168.0.0/16。 kubeadm init \\ --apiserver-advertise-address=10.9.56.239 \\ --image-repository registry.aliyuncs.com/google_containers \\ --control-plane-endpoint=cluster-endpoint \\ --upload-certs \\ --kubernetes-version v1.27.3 \\ --service-cidr=10.1.0.0/16 \\ --pod-network-cidr=192.168.0.0/16 \\ --v=5 部署成功后，输出如下： ... Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of control-plane nodes by copying certificate authorities and service account keys on each node and then running the following as root: kubeadm join cluster-endpoint:6443 --token 89si13.zc0n2y0d53td5wgs \\ --discovery-token-ca-cert-hash sha256:3d7be1719e14d6d1273a8184122093d0e6666174f2f62f890018fda618251bc5 \\ --control-plane \\ --certificate-key e467406274bac7ea4db433835f4b825d1f3614abe2be7d584965a5183c0b23e2 Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use kubeadm init phase upload-certs to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join cluster-endpoint:6443 --token 89si13.zc0n2y0d53td5wgs \\ --discovery-token-ca-cert-hash sha256:3d7be1719e14d6d1273a8184122093d0e6666174f2f62f890018fda618251bc5 master1 配置 kube config 环境变量 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # 设置环境变量 echo \"export KUBECONFIG=/etc/kubernetes/admin.conf\" \u003e\u003e ~/.bash_profile source ~/.bash_profile 查看 node 信息，状态为 NotReady [root@master1-10-9-56-239 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION master3 NotReady control-plane 2m50s v1.25.2 查看 kubelet 服务日志： journalctl -xeu kubelet 有如下报错： Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized 需要先安装 CNI 插件 安装 pod 网络插件 CNI：Calico 参考：Install Calico kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.24.3/manifests/tigera-operator.yaml kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.24.3/manifests/custom-resources.yaml 查看 pods、nodes 信息，状态均正常 [root@master1-10-9-56-239 ~]# kubectl get pods -A calico-apiserver calico-apiserver-bc5b75576-97k76 1/1 Running 0 11h calico-apiserver calico-apiserver-bc5b75576-xvm26 1/1 Running 0 11h calico-system calico-kube-controllers-5cb6bfc9bd-hcbk2 1/1 Running 0 11h calico-system calico-node-n44pc 0/1 Running 0 11h calico-system calico-typha-7cdd94c5d7-22xz8 1/1 Running 0 11h kube-system coredns-7bdc4cb885-6jsjg 1/1 Running 0 11h kube-system coredns-7bdc4cb885-wr2jh 1/1 Running 0 11h kube-system etcd-master1-10-9-125-21 1/1 Running 0 11h kube-system kube-apiserver-master1-10-9-56-239 1/1 Running 0 11h kube-system kube-controller-manager-master1-10-9-56-239 1/1 Running 1 (11h ago) 11h kube-system kube-proxy-94cjw 1/1 Running 0 11h kube-system kube-scheduler-master1-10-9-125-21 1/1 Running 1 (11h ago) 11h tigera-operator tigera-operator-5c5bbf4f78-n6","date":"2023-07-02","objectID":"/posts/k8s_deploy/:1:3","tags":["Kubernetes","Kubeadm"],"title":"使用 kubeadm 安装 k8s 集群","uri":"/posts/k8s_deploy/"},{"categories":["Kubernetes"],"content":"使用 kubeadm 创建高可用集群 参考：利用 kubeadm 创建高可用集群 说明1：master1 节点上执行 kubeadm init 成功后会输出加入其他节点的 join 命令，如： ... You can now join any number of control-plane nodes by copying certificate authorities and service account keys on each node and then running the following as root: kubeadm join cluster-endpoint:6443 --token 89si13.zc0n2y0d53td5wgs \\ --discovery-token-ca-cert-hash sha256:3d7be1719e14d6d1273a8184122093d0e6666174f2f62f890018fda618251bc5 \\ --control-plane \\ --certificate-key e467406274bac7ea4db433835f4b825d1f3614abe2be7d584965a5183c0b23e2 Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use kubeadm init phase upload-certs to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join cluster-endpoint:6443 --token 89si13.zc0n2y0d53td5wgs \\ --discovery-token-ca-cert-hash sha256:3d7be1719e14d6d1273a8184122093d0e6666174f2f62f890018fda618251bc5 说明2：kubeadm-certs Secret（证书）和解密密钥会在两个小时后失效，如果要重新上传证书并生成新的解密密钥，请在**已加入集群节点的控制平面上（如 master1）**使用以下命令： [root@master1-10-9-56-239 ~]# kubeadm init phase upload-certs --upload-certs [upload-certs] Storing the certificates in Secret \"kubeadm-certs\" in the \"kube-system\" Namespace [upload-certs] Using certificate key: e467406274bac7ea4db433835f4b825d1f3614abe2be7d584965a5183c0b23e2 说明3：如果 join 命令未保存，可以执行如下命令重新生成： 注意 生成的命令无 --certificate-key 、--control-plane 参数，可以执行 kubeadm init phase upload-certs --upload-certs 后，自行加上 --certificate-key。 如果加入的是 master 节点到集群，则需要指定 --control-plane 。 [root@master1-10-9-56-239 ~]# kubeadm token create --print-join-command kubeadm join cluster-endpoint:6443 --token udh68j.az1v5k24b7byp6bj --discovery-token-ca-cert-hash sha256:3d7be1719e14d6d1273a8184122093d0e6666174f2f62f890018fda618251bc5 部署 master2 节点 master2 上执行 join 命令加入集群 # 在 master2 节点上执行 kubeadm join cluster-endpoint:6443 --token udh68j.az1v5k24b7byp6bj \\ --discovery-token-ca-cert-hash sha256:3d7be1719e14d6d1273a8184122093d0e6666174f2f62f890018fda618251bc5 \\ --control-plane \\ --certificate-key e467406274bac7ea4db433835f4b825d1f3614abe2be7d584965a5183c0b23e2 部署成功后，输出如下： ... This node has joined the cluster and a new control plane instance was created: * Certificate signing request was sent to apiserver and approval was received. * The Kubelet was informed of the new secure connection details. * Control plane label and taint were applied to the new node. * The Kubernetes control plane instances scaled up. * A new etcd member was added to the local/stacked etcd cluster. To start administering your cluster from this node, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Run 'kubectl get nodes' to see this node join the cluster. master2 配置 kube config 环境变量 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # 设置环境变量 echo \"export KUBECONFIG=/etc/kubernetes/admin.conf\" \u003e\u003e ~/.bash_profile source ~/.bash_profile master2 确认 pods、nodes 状态 [root@master2-10-9-158-227 ~]# kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-flannel kube-flannel-ds-gqnxz 1/1 Running 0 33m kube-flannel kube-flannel-ds-wxxns 1/1 Running 0 21h kube-system coredns-c676cc86f-2kc85 1/1 Running 0 21h kube-system coredns-c676cc86f-qxq6b 1/1 Running 0 21h kube-system etcd-master1-10-9-56-239 1/1 Running 0 21h kube-system etcd-master2-10-9-158-227 1/1 Running 0 32m kube-system kube-apiserver-master1-10-9-56-239 1/1 Running 0 21h kube-system kube-apiserver-master2-10-9-158-227 1/1 Running 0 32m kube-system kube-controller-manager-master1-10-9-56-239 1/1 Running 0 21h kube-system kube-controller-manager-master2-10-9-158-227 1/1 Running 0 31m kube-system kube-proxy-czmhv 1/1 Running 0 21h kube-system kube-proxy-fmw87 1/1 Running 0 33m kube-system kube-scheduler-master","date":"2023-07-02","objectID":"/posts/k8s_deploy/:1:4","tags":["Kubernetes","Kubeadm"],"title":"使用 kubeadm 安装 k8s 集群","uri":"/posts/k8s_deploy/"},{"categories":["Kubernetes"],"content":"配置 kube-apiserver 高可用（所有master 节点上执行） 参考：软件负载平衡选项指南 选择基于 keepalived + haproxy 为 kube-apiserver 创建负载均衡器，并通过 k8s pod 的方式部署 keepalived 和 haproxy。 首先需要选定一个 VIP，本示例中 VIP 为：10.9.37.82 。 如果为云上环境，可以在 VPC 下单独申请 VIP。 初始化 keepalived 配置文件 keepalived 配置文件 /etc/keepalived/keepalived.conf： 修改配置： state：选择第一个节点为 MASTER，其他均为 BACKUP interface：即网卡，如 eth0 priority：优先级，MASTER 节点为 101，BACKUP 节点为 100 mkdir /etc/keepalived \u0026\u0026 cat \u003e\u003e /etc/keepalived/keepalived.conf\u003c\u003cEOF global_defs { router_id LVS_DEVEL } vrrp_script check_apiserver { script \"/etc/keepalived/check_apiserver.sh\" interval 3 weight -2 fall 10 rise 2 } vrrp_instance VI_1 { state MASTER # state BACKUP interface eth0 virtual_router_id 51 priority 101 # priority 100 authentication { auth_type PASS auth_pass 42 } virtual_ipaddress { 10.9.37.82/16 } track_script { check_apiserver } } EOF keepalived 健康检测脚本/etc/keepalived/check_apiserver.sh： 按实际修改 VIP 地址，如 10.9.37.82 #!/bin/sh errorExit() { echo \"*** $*\" 1\u003e\u00262 exit 1 } vip=10.9.37.82 curl --silent --max-time 2 --insecure https://localhost:8443/ -o /dev/null || errorExit \"Error GET https://localhost:8443/\" if ip addr | grep -q ${vip}; then curl --silent --max-time 2 --insecure https://${vip}:8443/ -o /dev/null || errorExit \"Error GET https://${vip}:8443/\" fi 初始化 haproxy 配置文件 haproxy 配置文件/etc/haproxy/haproxy.cfg： 修改配置： 修改 backend 配置中各 master 节点 IP 地址 mkdir /etc/haproxy \u0026\u0026 cat \u003e\u003e /etc/haproxy/haproxy.cfg\u003c\u003cEOF #--------------------------------------------------------------------- # Global settings #--------------------------------------------------------------------- global log /dev/log local0 log /dev/log local1 notice daemon #--------------------------------------------------------------------- # common defaults that all the 'listen' and 'backend' sections will # use if not designated in their block #--------------------------------------------------------------------- defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 1 timeout http-request 10s timeout queue 20s timeout connect 5s timeout client 20s timeout server 20s timeout http-keep-alive 10s timeout check 10s #--------------------------------------------------------------------- # apiserver frontend which proxys to the control plane nodes #--------------------------------------------------------------------- frontend apiserver bind *:8443 mode tcp option tcplog default_backend apiserver #--------------------------------------------------------------------- # round robin balancing for apiserver #--------------------------------------------------------------------- backend apiserver option httpchk GET /healthz http-check expect status 200 mode tcp option ssl-hello-chk balance roundrobin server k8s-master1 10.9.56.239:6443 check server k8s-master2 10.9.158.227:6443 check server k8s-master3 10.9.153.152:6443 check EOF 生成 Pod 清单文件 将 keepalived、haproxy Pod 清单文件放入 /etc/kubernetes/manifests 目录后，k8s 会监听目录文件变化，自动拉起 keepalived、haproxy Pod。 keepalived cat \u003e\u003e /etc/kubernetes/manifests/keepalived.yaml\u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: keepalived namespace: kube-system spec: containers: - image: osixia/keepalived:2.0.17 name: keepalived securityContext: capabilities: add: - NET_ADMIN - NET_BROADCAST - NET_RAW volumeMounts: - mountPath: /usr/local/etc/keepalived/keepalived.conf name: config - mountPath: /etc/keepalived/check_apiserver.sh name: check hostNetwork: true volumes: - hostPath: path: /etc/keepalived/keepalived.conf name: config - hostPath: path: /etc/keepalived/check_apiserver.sh name: check EOF haproxy cat \u003e\u003e /etc/kubernetes/manifests/haproxy.yaml\u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: haproxy namespace: kube-system spec: containers: - image: haproxy:2.1.4 name: haproxy livenessProbe: failureThreshold: 8 httpGet: host: localhost path: /healthz port: 8443 scheme: HTTPS volumeMounts: - mountPath: /usr/local/etc/haproxy/haproxy.cfg name: haproxyconf readOnly: true hostNetwork: true vo","date":"2023-07-02","objectID":"/posts/k8s_deploy/:1:5","tags":["Kubernetes","Kubeadm"],"title":"使用 kubeadm 安装 k8s 集群","uri":"/posts/k8s_deploy/"},{"categories":["Kubernetes"],"content":"添加 worker 节点 执行基础环境配置 章节各步骤，初始化节点 增加主机映射，如节点 IP 为 10.9.167.63（必须增加 cluster-endpoint 映射） cat \u003e\u003e /etc/hosts\u003c\u003cEOF 10.9.167.63 node-10-9-167-63 10.9.37.82 cluster-endpoint EOF 如未保存 join 命令，可在 master 节点重新生成 $\u003e kubeadm token create --print-join-command kubeadm join cluster-endpoint:6443 --token zs3ugs.6g3rygcf49ctxizq --discovery-token-ca-cert-hash sha256:9a851b28d96879f28027fa670e542dfa55a0c2564b7c6f8ff5c6473ef495b0f8 执行 kubectl join 命令添加节点 $\u003e kubeadm join cluster-endpoint:6443 --token zs3ugs.6g3rygcf49ctxizq --discovery-token-ca-cert-hash sha256:9a851b28d96879f28027fa670e542dfa55a0c2564b7c6f8ff5c6473ef495b0f8 [preflight] Running pre-flight checks [WARNING FileExisting-tc]: tc not found in system path [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml' [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Starting the kubelet [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster. ","date":"2023-07-02","objectID":"/posts/k8s_deploy/:1:6","tags":["Kubernetes","Kubeadm"],"title":"使用 kubeadm 安装 k8s 集群","uri":"/posts/k8s_deploy/"},{"categories":["Kubernetes"],"content":"删除 worker 节点 master 节点上执行节点删除 1) 驱逐节点上所有 pod $\u003e kubectl drain node-10-9-167-63 --delete-local-data --force --ignore-daemonsets 2) 确认 pod 全部驱逐 $\u003e kubectl get pods -A 3) 删除节点 $\u003e kubectl delete nodes node-10-9-167-63 清理节点上脏数据 $\u003e rm -rf /etc/kubernetes/* $\u003e systemctl stop kubelet.service ","date":"2023-07-02","objectID":"/posts/k8s_deploy/:1:7","tags":["Kubernetes","Kubeadm"],"title":"使用 kubeadm 安装 k8s 集群","uri":"/posts/k8s_deploy/"},{"categories":["Kubernetes"],"content":"问题记录 ","date":"2023-07-02","objectID":"/posts/k8s_deploy/:2:0","tags":["Kubernetes","Kubeadm"],"title":"使用 kubeadm 安装 k8s 集群","uri":"/posts/k8s_deploy/"},{"categories":["Kubernetes"],"content":"kubeadm init 报错 [root@10-9-66-153 ~]# kubeadm init --image-repository registry.aliyuncs.com/google_containers [init] Using Kubernetes version: v1.27.3 [preflight] Running pre-flight checks [WARNING FileExisting-tc]: tc not found in system path error execution phase preflight: [preflight] Some fatal errors occurred: [ERROR FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml]: /etc/kubernetes/manifests/kube-apiserver.yaml already exists [ERROR FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml]: /etc/kubernetes/manifests/kube-controller-manager.yaml already exists [ERROR FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml]: /etc/kubernetes/manifests/kube-scheduler.yaml already exists [ERROR FileAvailable--etc-kubernetes-manifests-etcd.yaml]: /etc/kubernetes/manifests/etcd.yaml already exists [ERROR Port-10250]: Port 10250 is in use [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...` To see the stack trace of this error execute with --v=5 or higher 之前已经运行过多次 kubeadm init，导致 yaml 文件冲突，可以增加 --ignore-preflight-errors=all 参数忽略错误继续执行；或者先重置（kubeadm reset）之后，再执行 kubeadm init 。 kubeadm reset rm -rf ~/.kube/ /etc/kubernetes/* var/lib/etcd/* ","date":"2023-07-02","objectID":"/posts/k8s_deploy/:2:1","tags":["Kubernetes","Kubeadm"],"title":"使用 kubeadm 安装 k8s 集群","uri":"/posts/k8s_deploy/"},{"categories":["Kubernetes"],"content":"flannel pod 启动异常 [root@master3 ~]# kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-flannel kube-flannel-ds-hxtgs 0/1 CrashLoopBackOff 3 (21s ago) 86s kube-system coredns-c676cc86f-9wkz6 0/1 ContainerCreating 0 5m6s kube-system coredns-c676cc86f-rq4g9 0/1 ContainerCreating 0 5m6s kube-system etcd-master3 1/1 Running 1 5m11s kube-system kube-apiserver-master3 1/1 Running 1 5m12s kube-system kube-controller-manager-master3 1/1 Running 1 5m11s kube-system kube-proxy-wnqcw 1/1 Running 0 5m6s kube-system kube-scheduler-master3 1/1 Running 1 5m11s 查看 pod 日志 [root@master3 ~]# kubectl logs pod -n kube-flannel kube-flannel-ds-hxtgs Error from server (NotFound): pods \"pod\" not found [root@master3 ~]# kubectl logs -n kube-flannel kube-flannel-ds-hxtgs Defaulted container \"kube-flannel\" out of: kube-flannel, install-cni-plugin (init), install-cni (init) I0701 05:34:55.161599 1 main.go:204] CLI flags config: {etcdEndpoints:http://127.0.0.1:4001,http://127.0.0.1:2379 etcdPrefix:/coreos.com/network etcdKeyfile: etcdCertfile: etcdCAFile: etcdUsername: etcdPassword: version:false kubeSubnetMgr:true kubeApiUrl: kubeAnnotationPrefix:flannel.alpha.coreos.com kubeConfigFile: iface:[] ifaceRegex:[] ipMasq:true ifaceCanReach: subnetFile:/run/flannel/subnet.env publicIP: publicIPv6: subnetLeaseRenewMargin:60 healthzIP:0.0.0.0 healthzPort:0 iptablesResyncSeconds:5 iptablesForwardRules:true netConfPath:/etc/kube-flannel/net-conf.json setNodeNetworkUnavailable:true} W0701 05:34:55.161658 1 client_config.go:617] Neither --kubeconfig nor --master was specified. Using the inClusterConfig. This might not work. I0701 05:34:55.259826 1 kube.go:126] Waiting 10m0s for node controller to sync I0701 05:34:55.259883 1 kube.go:420] Starting kube subnet manager I0701 05:34:56.260010 1 kube.go:133] Node controller sync successful I0701 05:34:56.260036 1 main.go:224] Created subnet manager: Kubernetes Subnet Manager - master3 I0701 05:34:56.260045 1 main.go:227] Installing signal handlers I0701 05:34:56.260138 1 main.go:467] Found network config - Backend type: vxlan I0701 05:34:56.260153 1 match.go:206] Determining IP address of default interface I0701 05:34:56.260542 1 match.go:259] Using interface with name eth0 and address 10.9.88.158 I0701 05:34:56.260568 1 match.go:281] Defaulting external address to interface address (10.9.88.158) I0701 05:34:56.260620 1 vxlan.go:138] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false E0701 05:34:56.260846 1 main.go:327] Error registering network: failed to acquire lease: node \"master3\" pod cidr not assigned I0701 05:34:56.260914 1 main.go:447] Stopping shutdownHandler... Error registering network: failed to acquire lease: node \"master3\" pod cidr not assigned kubeadm init 时需要指定参数 --pod-network-cidr=10.244.0.0/16 ","date":"2023-07-02","objectID":"/posts/k8s_deploy/:2:2","tags":["Kubernetes","Kubeadm"],"title":"使用 kubeadm 安装 k8s 集群","uri":"/posts/k8s_deploy/"},{"categories":["Hugo 博客"],"content":"Hugo + Github Pages 搭建个人博客 ","date":"2019-05-19","objectID":"/posts/hugo_github_pages/:1:0","tags":["Hugo","Github Pages","LoveIt"],"title":"Hugo + Github Pages 搭建个人博客","uri":"/posts/hugo_github_pages/"},{"categories":["Hugo 博客"],"content":"环境安装 安装 Hugo 官方安装说明：https://gohugo.io/installation/ brew install hugo 初始化博客项目仓库 hugo new site mblog cd mblog git init 配置 Hugo 主题 Hugo 博客框架有很多开源的主题，官方列表：https://themes.gohugo.io/ 以 LoveIt 为例，安装主题： # 进入项目根目录 cd mblog # 安装主题 git submodule add https://github.com/dillonzq/LoveIt.git themes/LoveIt 修改项目根目录下配置文件 config.toml，修改 theme 配置，如： theme = \"LoveIt\" 添加一篇文章 hugo new posts/first_post.md 新建一篇名为 first_post.md 的文章，存放在 mblog/content/posts。之后可以使用 Markdown 编辑器直接修改 first_post.md。 本地启动服务预览 hugo server -D 服务启动后，浏览器访问 http://localhost:1313/，查看网站效果。 同时，可以使用如下命令指定服务进程的 IP、端口： hugo server -D --bind 192.168.187.176 --port 5555 构建网站 # 进入项目根目录 cd mblog # 构建网站 hugo 执行 hugo 命令后，会将网站静态文件编译到 mblog/public 目录下。 ","date":"2019-05-19","objectID":"/posts/hugo_github_pages/:1:1","tags":["Hugo","Github Pages","LoveIt"],"title":"Hugo + Github Pages 搭建个人博客","uri":"/posts/hugo_github_pages/"},{"categories":["Hugo 博客"],"content":"GitHub 关联 GitHub 创建个人网站仓库 登录到 GitHub，点击 New repository 创建新仓库，仓库名指定为为：用户名.github.io 。 用户名为 GitHub 帐号名称，比如我的 GitHub 名称为benzimu ，所以仓库名为：benzimu.github.io。 网站关联 GitHub 仓库 cd mblog/public git init git remote add origin git@github.com:benzimu/benzimu.github.io.git 部署网站 cd mblog/public git add . git commit -m \"first commit\" git push origin master 发布之后，可以在浏览器访问 https://benzimu.github.io/ 地址，就能看到我们的网站了。 提交网站源码到 Github 登录到 GitHub，点击 New repository 创建新仓库，仓库名称为：mblog ，注意选择私有仓库。 仓库关联 cd mblog git remote add origin git@github.com:benzimu/mblog.git 增加 .gitignore 文件 cat \u003e mblog/.gitignore \u003c\u003c EOF .DS_Store Thumbs.db db.json *.log node_modules/ public/ .deploy*/ _multiconfig.yml EOF 提交代码 cd mblog git add . git commit -m \"first commit\" git push origin master ","date":"2019-05-19","objectID":"/posts/hugo_github_pages/:1:2","tags":["Hugo","Github Pages","LoveIt"],"title":"Hugo + Github Pages 搭建个人博客","uri":"/posts/hugo_github_pages/"},{"categories":["Hugo 博客"],"content":"网站部署脚本 可将该脚本放在 mblog 根目录下，每次修改博客后，直接执行 bash deploy.sh 即可自动发布 cat \u003e mblog/deploy.sh \u003c\u003c EOF #!/bin/bash sed -e echo -e \"\\033[0;32mDeploying updates to GitHub...\\033[0m\" # 发布：编译静态文件到 public 目录，如 HTML、images, CSS, and JavaScript hugo # 部署：将静态文件上传到 github cd public || exit git add . msg=\"rebuilding site $(date \"+%Y%m%d%H%M%S\")\" echo -e \"\\033[0;32m$msg\\033[0m\" if [ $# -eq 1 ]; then msg=\"$1\" fi git commit -m \"$msg\" git push origin master cd .. echo \"Deploy successful\" EOF ","date":"2019-05-19","objectID":"/posts/hugo_github_pages/:1:3","tags":["Hugo","Github Pages","LoveIt"],"title":"Hugo + Github Pages 搭建个人博客","uri":"/posts/hugo_github_pages/"},{"categories":["Hugo 博客"],"content":"个人域名配置 配置个人域名解析规则 benzimu.github.io 为 GitHub 提供的访问地址，可以通过设置绑定自己的域名。首先需要购买一个域名，如 benzimu.com 。 获取 GitHub 域名 IP 地址 每个项目分配的 IP 可能不同 $\u003e nslookup benzimu.github.io Server: 192.168.138.111 Address: 192.168.138.111#53 Non-authoritative answer: Name: benzimu.github.io Address: 185.199.111.153 Name: benzimu.github.io Address: 185.199.110.153 Name: benzimu.github.io Address: 185.199.109.153 Name: benzimu.github.io Address: 185.199.108.153 个人域名配置解析规则 登录到域名服务方（如阿里）控制台，配置一条 CNAME 类型记录指向 benzimu.github.io，以及多条 A 类型记录指向具体 IP 地址。 域名解析规则 GitHub 配置个人域名 登录 GitHub，访问benzimu.github.io 仓库，修改配置Settings -\u003e Pages -\u003e Custom domain 填写自己的域名，并开启 HTTPS 访问。 Github Pages 设置 域名解析有一定时延，过段时间后，即可通过个人域名访问网站了。 ","date":"2019-05-19","objectID":"/posts/hugo_github_pages/:1:4","tags":["Hugo","Github Pages","LoveIt"],"title":"Hugo + Github Pages 搭建个人博客","uri":"/posts/hugo_github_pages/"},{"categories":["Hugo 博客"],"content":"参考 https://gohugo.io/getting-started/quick-start/ https://hugoloveit.com/zh-cn/theme-documentation-basics/ ","date":"2019-05-19","objectID":"/posts/hugo_github_pages/:1:5","tags":["Hugo","Github Pages","LoveIt"],"title":"Hugo + Github Pages 搭建个人博客","uri":"/posts/hugo_github_pages/"},{"categories":["Tornado 源码解析"],"content":"tornado multiple processes tornado 实现了多进程的执行模式。使用 tornado 多进程启动服务时，IOLoop 的初始化（IOLoop.current()）操作必须在 fork 子进程之后执行。且在多进程模式下无法使用 debug 模式，因为实现 debug 模式的 autoreload.py 在 tornado.web.Application 初始化时就已经实例化了 IOLoop。tornado 使用多进程的方式有下面两种： # 方式一 app = tornado.web.Application(urls) sockets = tornado.netutil.bind_sockets(port, address) tornado.process.fork_processes(0) # # 默认为系统CPU核数 http_server = tornado.httpserver.HTTPServer(app) http_server.add_sockets(sockets) tornado.ioloop.IOLoop.current().start() # 方式二 app = tornado.web.Application(urls) server = tornado.httpserver.HTTPServer(app) server.bind(port, address) server.start(0) # 默认为系统CPU核数 tornado.ioloop.IOLoop.current().start() 以方式一为例分析多进程实现源码。tornado.netutil.bind_sockets() 详解参考：tornado_netutil#bind_sockets() ","date":"2017-11-10","objectID":"/posts/tornado_multi_processes/:1:0","tags":["Tornado"],"title":"tornado 多进程实现解析","uri":"/posts/tornado_multi_processes/"},{"categories":["Tornado 源码解析"],"content":"tornado.process.fork_processes() def fork_processes(num_processes, max_restarts=100): global _task_id assert _task_id is None # 当传入的num_processes为None或者小于等于0时，默认使用系统CPU核数 if num_processes is None or num_processes \u003c= 0: num_processes = cpu_count() # 如果IOLoop已经实例化，则抛出异常 if ioloop.IOLoop.initialized(): raise RuntimeError(\"Cannot run in multiple processes: IOLoop instance \" \"has already been initialized. You cannot call \" \"IOLoop.instance() before calling start_processes()\") gen_log.info(\"Starting %d processes\", num_processes) # 用于保存子进程PID children = {} # fork子进程的内部函数 def start_child(i): # 一次调用，两次返回 pid = os.fork() # 子进程运行 if pid == 0: # child process _reseed_random() global _task_id _task_id = i return i # 父进程运行 else: children[pid] = i return None # 根据num_processes生成对应数量的子进程 for i in range(num_processes): id = start_child(i) # 此时父进程还需要执行下面的操作，所以fork时必须父进程先执行，否者直接返回 if id is not None: return id num_restarts = 0 # 遍历所有子进程 while children: try: # 等待子进程执行，阻塞，返回子进程的pid、子进程退出时的状态信息，0表示子进程没有出现异常 pid, status = os.wait() except OSError as e: if errno_from_exception(e) == errno.EINTR: continue raise if pid not in children: continue # 将退出的进程从dict中移除 id = children.pop(pid) # 处理子进程退出状态 if os.WIFSIGNALED(status): gen_log.warning(\"child %d (pid %d) killed by signal %d, restarting\", id, pid, os.WTERMSIG(status)) elif os.WEXITSTATUS(status) != 0: gen_log.warning(\"child %d (pid %d) exited with status %d, restarting\", id, pid, os.WEXITSTATUS(status)) else: gen_log.info(\"child %d (pid %d) exited normally\", id, pid) continue num_restarts += 1 # 如果重启次数操作最大次数，则抛出异常 if num_restarts \u003e max_restarts: raise RuntimeError(\"Too many child restarts, giving up\") # 重启子进程 new_id = start_child(id) if new_id is not None: return new_id sys.exit(0) os.fork() 函数调用有两次返回，当返回值为 0 时，表示此时为子进程运行中，当值不为 0 时，表示父进程运行中。因为父进程需要继续执行下面的代码，所以 fork() 子进程返回时必须先执行父进程，即返回值不为 0。调用 fork() 之后先执行哪个进程的是由 Linux 下专有文件 /proc/sys/kernel/sched_child_runs_first 的值来确定的(值为 0 父进程先执行，非 0 子进程先执行)。 当子进程全部 fork() 完成之后，父进程（main 函数）会调用 os.wait() 阻塞住，等待子进程执行。此时，所有的子进程会执行各自内存空间的代码段。子进程由于完全复用父进程的代码段，则都会继续执行方式一中 tornado.process.fork_processes(0) 之后的代码。只有当所有的子进程都正常退出或者重启次数超过限制之后，父进程才会退出（sys.exit(0)）。 http_server.add_sockets(sockets) 方法会完成服务器 socket 的监听，即 accept()，并将其回调函数注册到 IOLoop，已完成客户端与服务端的通信。详解请参考：tornado_httpserver。 方式二的 server.start(0) 封装了子进程的fork操作，原理与方式一一样。 ","date":"2017-11-10","objectID":"/posts/tornado_multi_processes/:1:1","tags":["Tornado"],"title":"tornado 多进程实现解析","uri":"/posts/tornado_multi_processes/"},{"categories":["Tornado 源码解析"],"content":"多进程模式下，如何避免同一个请求不被多次执行呢？ tornado 多进程的处理流程实现创建服务器 socket，然后在 fork 子进程，这样所有的子进程都监听同一个文件描述符，即同一个 socket。 当连接过来时，所有的子进程都会收到可读事件，这是所有的子进程都会调到 accept_handler 回调函数，尝试建立连接。 一旦其中的一个子进程成功的建立了连接，当其他子进程在尝试建立连接时就会触发EWOULDBLOCK或者EAGAIN错误，这时回调函数判断是这个错误则不做处理。详解参考：tornado_netutil#add_accept_handler()。 当成功建立连接的子进程还在处理这个连接的时候有过来一个连接，则由另外一个子进程处理这个连接。 tornado 就是通过这样一种机制，利用多进程提升效率，由于一个连接只能有一个子进程成功创建，同一个请求也就不会被多个子进程处理。 IOLoop 为单例、多进程模式。 ","date":"2017-11-10","objectID":"/posts/tornado_multi_processes/:1:2","tags":["Tornado"],"title":"tornado 多进程实现解析","uri":"/posts/tornado_multi_processes/"},{"categories":["Tornado 源码解析"],"content":"tornado concurrent 实现解析 用于处理线程和 Futures 的工具。Futures 是 python3.2 中 concurrent.futures 包引入的并发编程模式。在本模块中，定义了一个兼容的 Future 类，它被设计用于协同工作，还有一些用于与 concurrent.futures 包交互的实用函数。 Future 的设计目标是作为协程（coroutine）和 IOLoop 的媒介，从而将协程和 IOLoop 关联起来。 Future 是异步操作结果的占位符，用于等待结果返回。通常作为函数 IOLoop.add_future() 的参数或 gen.coroutine 协程中 yield 的返回值。 等到结果返回时，外部可以通过调用 set_result() 设置真正的结果，将结果保存在 Future 内存中，然后调用所有回调函数，恢复协程的执行，最后通过 result() 获取结果。 Future 类通过 self._done 的值来判断本次 Future 操作是否结束，默认为 False，当调用 set_result() 设置结果、或者调用 set_exc_info() 设置异常时会调用 _set_done() 修改其值，即表示本次操作已经完成。 ","date":"2017-11-10","objectID":"/posts/tornado_concurrent/:1:0","tags":["Tornado"],"title":"tornado concurrent 实现解析","uri":"/posts/tornado_concurrent/"},{"categories":["Tornado 源码解析"],"content":"tornado.concurrent.Future.set_result() def set_result(self, result): # 将结果保存到 self._result self._result = result # 修改 self._done，并调用所有回调函数 self._set_done() ","date":"2017-11-10","objectID":"/posts/tornado_concurrent/:1:1","tags":["Tornado"],"title":"tornado concurrent 实现解析","uri":"/posts/tornado_concurrent/"},{"categories":["Tornado 源码解析"],"content":"tornado.concurrent.Future._set_done() def _set_done(self): # 修改 self._done 值，表示本次操作已完成 self._done = True # 循环执行 self._callbacks 中的回调函数，通过 add_done_callback() 定义 for cb in self._callbacks: try: cb(self) except Exception: app_log.exception('Exception in callback %r for %r', cb, self) self._callbacks = None ","date":"2017-11-10","objectID":"/posts/tornado_concurrent/:1:2","tags":["Tornado"],"title":"tornado concurrent 实现解析","uri":"/posts/tornado_concurrent/"},{"categories":["Tornado 源码解析"],"content":"tornado.concurrent.Future.add_done_callback() def add_done_callback(self, fn): # 添加本次 Future 操作完成时的回调函数，如果 Future 还没结束，则将回调函数加入 # self._callbacks，以便结束时调用，如果已经结束，则直接运行回调函数 if self._done: fn(self) else: self._callbacks.append(fn) ","date":"2017-11-10","objectID":"/posts/tornado_concurrent/:1:3","tags":["Tornado"],"title":"tornado concurrent 实现解析","uri":"/posts/tornado_concurrent/"},{"categories":["Tornado 源码解析"],"content":"tornado.concurrent.Future.result() def result(self, timeout=None): # 清理日志 self._clear_tb_log() # 判断 self._result 结果是否为None，通过 set_result() 设置 if self._result is not None: return self._result # 判断是否有异常信息，通过 set_exc_info() 设置 if self._exc_info is not None: try: raise_exc_info(self._exc_info) finally: self = None # 检查是否结束 self._check_done() return self._result ","date":"2017-11-10","objectID":"/posts/tornado_concurrent/:1:4","tags":["Tornado"],"title":"tornado concurrent 实现解析","uri":"/posts/tornado_concurrent/"},{"categories":["Tornado 源码解析"],"content":"tornado.concurrent.chain_future() def chain_future(a, b): # 将两个Future对象关联在一起，一个完成，另一个也完成 # “a”的结果（成功或失败）将被复制到“b”，除非在“a”结束之前，“b”已经完成或被取消。 def copy(future): assert future is a # 如果b已经完成或者取消，则直接返回 if b.done(): return if (isinstance(a, TracebackFuture) and isinstance(b, TracebackFuture) and a.exc_info() is not None): b.set_exc_info(a.exc_info()) elif a.exception() is not None: b.set_exception(a.exception()) else: b.set_result(a.result()) # 将 copy()添加到 a 的回调列表中 a.add_done_callback(copy) 首先会调用 a.add_done_callback(copy)，若 a 已经完成则直接运行 copy 函数，否则加入到回调列表中等到 a 完成时再运行。copy 函数将两个 Future 对象联系到了一起，用作结果返回、超时处理。 ","date":"2017-11-10","objectID":"/posts/tornado_concurrent/:1:5","tags":["Tornado"],"title":"tornado concurrent 实现解析","uri":"/posts/tornado_concurrent/"},{"categories":["Tornado 源码解析"],"content":"tornado gen 实现解析 tornado.gen是一个基于生成器的接口，可以更容易地在异步环境中工作。使用gen模块的代码在技术上是异步的，但它被写成一个单独的生成器，而不是另外的函数集合。 ","date":"2017-11-10","objectID":"/posts/tornado_gen/:1:0","tags":["Tornado"],"title":"tornado gen 实现解析","uri":"/posts/tornado_gen/"},{"categories":["Tornado 源码解析"],"content":"tornado.gen.with_timeout() def with_timeout(timeout, future, io_loop=None, quiet_exceptions=()): # 在超时时间内封装Future对象 # 如果传入的future在超时之前没有完成，则引发TimeoutError，可以通过 # .IOLoop.add_timeout（即datetime.timedelta或者相对于.IOLoop.time的绝对 # 时间）允许的任何形式来指定。 # 如果封装的Future在超时之后失败，则将记录该异常，除非它是“quiet_exceptions” # （可能是一个异常类型或一系列类型）中包含的类型。 # 将一个yielded对象转换为一个Future future = convert_yielded(future) # 初始化一个新的Future对象 result = Future() # 将新的Future对象与待处理的future关联，用于超时处理、结果处理 chain_future(future, result) if io_loop is None: io_loop = IOLoop.current() # future超时处理函数 def error_callback(future): try: future.result() except Exception as e: if not isinstance(e, quiet_exceptions): app_log.error(\"Exception in Future %r after timeout\", future, exc_info=True) # IOLoop超时回调函数 def timeout_callback(): result.set_exception(TimeoutError(\"Timeout\")) # In case the wrapped future goes on to fail, log it. future.add_done_callback(error_callback) # 添加超时事件到IOLoop timeout_handle = io_loop.add_timeout( timeout, timeout_callback) # 根据future不同类型，分别处理删除IOLoop中future的超时回调事件 if isinstance(future, Future): # We know this future will resolve on the IOLoop, so we don't # need the extra thread-safety of IOLoop.add_future (and we also # don't care about StackContext here. future.add_done_callback( lambda future: io_loop.remove_timeout(timeout_handle)) else: # concurrent.futures.Futures may resolve on any thread, so we # need to route them back to the IOLoop. io_loop.add_future( future, lambda future: io_loop.remove_timeout(timeout_handle)) return result 首先会初始化一个 Future 对象，用于获取结果，即 result。 然后调用 chain_future 将 future 与 result 对象绑定，参考详解：tornado_concurrent#chain_future()。 接着会注册超时事件到 IOLoop，具体参考详解：tornado_ioloop_PeriodicCallback#add_timeout()。 如果 timeout 超时了，则 IOLoop 会调用 timeout_callback，然后 result 调用 set_exception 抛出异常，并终止 Future，即设置 self._done 为 True；future 也同样抛出异常，会将异常写入日志中。如果在 timeout 超时之前 future 操作成功完成，则会将其返回数据写到 result 中，并删除注册到 IOLoop 中的超时回调事件。 ","date":"2017-11-10","objectID":"/posts/tornado_gen/:1:1","tags":["Tornado"],"title":"tornado gen 实现解析","uri":"/posts/tornado_gen/"},{"categories":["Tornado 源码解析"],"content":"tornado 定时器 PeriodicCallback tornado.ioloop.PeriodicCallback 是 tornado 实现的定时器。 class Application(tornado.web.Application): def __init__(self): handlers = [ (r\"/\", HomeHandler), ] settings = dict( debug=True, ) super(Application, self).__init__(handlers, **settings) 当创建 tornado Application 时，如果设置 “debug=True”，tornado 会在源程序修改后自动编译，而不需要我们手动重启。 ","date":"2017-11-10","objectID":"/posts/tornado_ioloop_periodiccallback/:1:0","tags":["Tornado"],"title":"tornado 定时器 PeriodicCallback 实现解析","uri":"/posts/tornado_ioloop_periodiccallback/"},{"categories":["Tornado 源码解析"],"content":"tornado.web.Application.__init__() def __init__(self, handlers=None, default_host=None, transforms=None, **settings): #####省略##### # 当设置了debug=True时，tornado会默认设置autoreload=True if self.settings.get('debug'): self.settings.setdefault('autoreload', True) self.settings.setdefault('compiled_template_cache', False) self.settings.setdefault('static_hash_cache', False) self.settings.setdefault('serve_traceback', True) #####省略##### # 判断是否设置了autoreload，如果设置了就会调用autoreload.start()， # 启动自动重载功能 if self.settings.get('autoreload'): from tornado import autoreload autoreload.start() ","date":"2017-11-10","objectID":"/posts/tornado_ioloop_periodiccallback/:1:1","tags":["Tornado"],"title":"tornado 定时器 PeriodicCallback 实现解析","uri":"/posts/tornado_ioloop_periodiccallback/"},{"categories":["Tornado 源码解析"],"content":"tornado.autoreload.start() def start(io_loop=None, check_time=500): # 获取IOLoop实例 io_loop = io_loop or ioloop.IOLoop.current() # 如果当前IOLoop实例是弱引用的则直接返回 if io_loop in _io_loops: return _io_loops[io_loop] = True if len(_io_loops) \u003e 1: gen_log.warning(\"tornado.autoreload started more than once in the same process\") modify_times = {} # 生成回调函数 callback = functools.partial(_reload_on_update, modify_times) # 初始化tornado.PeriodicCallback定时器 scheduler = ioloop.PeriodicCallback(callback, check_time, io_loop=io_loop) # 开始执行定时器 scheduler.start() ","date":"2017-11-10","objectID":"/posts/tornado_ioloop_periodiccallback/:1:2","tags":["Tornado"],"title":"tornado 定时器 PeriodicCallback 实现解析","uri":"/posts/tornado_ioloop_periodiccallback/"},{"categories":["Tornado 源码解析"],"content":"tornado.ioloop.PeriodicCallback.start() def start(self): # 设置定时器运行中 self._running = True # 初始化下次的超时事件的最后期限 self._next_timeout = self.io_loop.time() # 关键方法，对下次超时事件的封装 self._schedule_next() ","date":"2017-11-10","objectID":"/posts/tornado_ioloop_periodiccallback/:1:3","tags":["Tornado"],"title":"tornado 定时器 PeriodicCallback 实现解析","uri":"/posts/tornado_ioloop_periodiccallback/"},{"categories":["Tornado 源码解析"],"content":"tornado.ioloop.PeriodicCallback._schedule_next() def _schedule_next(self): if self._running: # 获取当前时间 current_time = self.io_loop.time() # 如果当前时间已经超过了超时事件的最后期限，则重新设置超时时间 if self._next_timeout \u003c= current_time: # self.callback_time为autoreload.start()方法中初始化定时器时传入的 # check_time，即500毫秒 callback_time_sec = self.callback_time / 1000.0 self._next_timeout += (math.floor((current_time - self._next_timeout) / callback_time_sec) + 1) * callback_time_sec # 关键所在，添加超时事件，将self._run作为超时后的回调函数 self._timeout = self.io_loop.add_timeout(self._next_timeout, self._run) ","date":"2017-11-10","objectID":"/posts/tornado_ioloop_periodiccallback/:1:4","tags":["Tornado"],"title":"tornado 定时器 PeriodicCallback 实现解析","uri":"/posts/tornado_ioloop_periodiccallback/"},{"categories":["Tornado 源码解析"],"content":"tornado.ioloop.PeriodicCallback._run() def _run(self): # 判断定时器是否还在运行 if not self._running: return try: # 调用autoreload.start()方法中初始化时传入的超时回调函数 return self.callback() except Exception: self.io_loop.handle_callback_exception(self.callback) finally: # 无论如何，都会再次调用self._schedule_next()再次添加超时事件到IOLoop中， # 这样就会一直循环，即定时器操作 self._schedule_next() 在 PeriodicCallback._schedule_next() 的最后一行执行的添加超时事件就会被 IOLoop 下次循环中。 ","date":"2017-11-10","objectID":"/posts/tornado_ioloop_periodiccallback/:1:5","tags":["Tornado"],"title":"tornado 定时器 PeriodicCallback 实现解析","uri":"/posts/tornado_ioloop_periodiccallback/"},{"categories":["Tornado 源码解析"],"content":"tornado.ioloop.IOLoop.add_timeout() def add_timeout(self, deadline, callback, *args, **kwargs): # 判断超时事件的最后期限deadline是否为实数，一般为实数 if isinstance(deadline, numbers.Real): return self.call_at(deadline, callback, *args, **kwargs) # 在使用call_later()方法设置超时事件时deadline为datetime.timedelta类型 elif isinstance(deadline, datetime.timedelta): return self.call_at(self.time() + timedelta_to_seconds(deadline), callback, *args, **kwargs) else: raise TypeError(\"Unsupported deadline %r\" % deadline) tornado 规定继承至 IOLoop 的子类必须实现 add_timeout() 或者 call_at() 方法，因为默认实现只是相互调用，而没有实质作用。tornado.ioloop.PollIOLoop 则实现了 call_at()。 ","date":"2017-11-10","objectID":"/posts/tornado_ioloop_periodiccallback/:1:6","tags":["Tornado"],"title":"tornado 定时器 PeriodicCallback 实现解析","uri":"/posts/tornado_ioloop_periodiccallback/"},{"categories":["Tornado 源码解析"],"content":"tornado.ioloop.PollIOLoop.call_at() def call_at(self, deadline, callback, *args, **kwargs): # 初始化_Timeout对象，该对象是对超时事件的封装，同时重写了__lt__()、__le__() # 两个方法，实现了_Timeout的大小比较。 timeout = _Timeout( deadline, functools.partial(stack_context.wrap(callback), *args, **kwargs), self) # 通过堆排序添加timeout到self._timeouts列表中，因此确定了self._timeouts[0] # 总是最小的，即最后期限deadline最小的，当最后期限相同时则为最先添加 # 到self._timeouts的 heapq.heappush(self._timeouts, timeout) return timeout 执行该函数之后 timeouts 就会被添加到 self._timeouts 中，当 tornado IOLoop 的 epoll.poll() 函数再次醒来时，则会重新迭代，然后调用 self._timeouts，进行相关判断处理。详解参考：tornado_ioloop#start() ","date":"2017-11-10","objectID":"/posts/tornado_ioloop_periodiccallback/:1:7","tags":["Tornado"],"title":"tornado 定时器 PeriodicCallback 实现解析","uri":"/posts/tornado_ioloop_periodiccallback/"},{"categories":["Tornado 源码解析"],"content":"tornado iostream 实现解析 封装了对 socket fd 底层数据的读取、写入操作，针对不同的情况实现了几种不同的读取方式。 ","date":"2017-11-06","objectID":"/posts/tornado_iostream/:1:0","tags":["Tornado"],"title":"tornado iostream 实现解析","uri":"/posts/tornado_iostream/"},{"categories":["Tornado 源码解析"],"content":"tornado.iostream.BaseIOStream.read_until_regex() def read_until_regex(self, regex, callback=None, max_bytes=None): # 设置读取完成后的回调函数 future = self._set_read_callback(callback) # 编译正则表达式，并保存到self._read_regex self._read_regex = re.compile(regex) # 保存最大读取数据长度到self._read_max_bytes self._read_max_bytes = max_bytes try: self._try_inline_read() except UnsatisfiableReadError as e: # Handle this the same way as in _handle_events. gen_log.info(\"Unsatisfiable read, closing connection: %s\" % e) self.close(exc_info=True) return future except: if future is not None: # Ensure that the future doesn't log an error because its # failure was never examined. future.add_done_callback(lambda f: f.exception()) raise return future 通过正则表达式匹配客户端发来的数据，http协议规定了数据结构，每行数据后面都会有一个 CRLF，而请求行与请求头数据结尾也会有一个 CRLF，即请求头数据后面有两个 CRLF，请求体也是如此。 CRLF 即为回车（Carriage-Return，CR，\\r）、换行（Line-Feed，LF，\\n）。Windows 下 CRLF 表示：\\r\\n；Unix 下 CRLF 表示：\\n。 tornado.http1connection.HTTP1Connection._read_message() 方法调用本方法传入的 regex 参数为 “\\r?\\n\\r?\\n”，刚好兼容了 Windows 和 Unix。 ","date":"2017-11-06","objectID":"/posts/tornado_iostream/:1:1","tags":["Tornado"],"title":"tornado iostream 实现解析","uri":"/posts/tornado_iostream/"},{"categories":["Tornado 源码解析"],"content":"tornado.iostream.BaseIOStream._set_read_callback() def _set_read_callback(self, callback): assert self._read_callback is None, \"Already reading\" assert self._read_future is None, \"Already reading\" if callback is not None: self._read_callback = stack_context.wrap(callback) else: self._read_future = TracebackFuture() return self._read_future 通过 read_until_regex 方法读取请求数据时，是没有 callback 的，即 callback 为 None，所以_set_read_callback 会返回一个 Future 对象实例，用于返回异步执行结果。 ","date":"2017-11-06","objectID":"/posts/tornado_iostream/:1:2","tags":["Tornado"],"title":"tornado iostream 实现解析","uri":"/posts/tornado_iostream/"},{"categories":["Tornado 源码解析"],"content":"tornado.iostream.BaseIOStream._try_inline_read() def _try_inline_read(self): # 尝试从缓存数据中完成当前读操作。 # 如果此次读操作能在无阻塞的情况下完成，则在下次IOLoop迭代中执行读回调函数， # 否则为此次读事件在套接字上启动监听 # 查看是否从前一次的读操作中获取到了数据 # 第一部分 self._run_streaming_callback() pos = self._find_read_pos() if pos is not None: self._read_from_buffer(pos) return # 第二部分 self._check_closed() try: pos = self._read_to_buffer_loop() except Exception: # If there was an in _read_to_buffer, we called close() already, # but couldn't run the close callback because of _pending_callbacks. # Before we escape from this function, run the close callback if # applicable. self._maybe_run_close_callback() raise if pos is not None: self._read_from_buffer(pos) return # 第三部分 # We couldn't satisfy the read inline, so either close the stream # or listen for new data. if self.closed(): self._maybe_run_close_callback() else: self._add_io_state(ioloop.IOLoop.READ) 方法可以分为三部分： 第一部分 首先会去检测前一次是否将数据读取到了缓存，即通过 self._find_read_pos() 获取到的 pos 是否为 None，如果不为 None，则表示已经读取数据完成（读取到足够大小的数据：max_bytes 参数、正则表达式匹配成功：regex 参数，或者遇到指定的分隔符：read_until() 方法中的 delimiter 参数），然后调用 self._read_from_buffer(pos) 将_read_future 加入 IOLoop； 第二部分 调用 self._read_to_buffer_loop() 循环读取请求数据，方法中封装了第一部分的部分操作，接下来与第一部分操作一致； 第三部分 判断该stream连接是否断开，如果已经断开调用关闭回调函数，否则将该连接再次放到 IOLoop 中，继续读取数据。 ","date":"2017-11-06","objectID":"/posts/tornado_iostream/:1:3","tags":["Tornado"],"title":"tornado iostream 实现解析","uri":"/posts/tornado_iostream/"},{"categories":["Tornado 源码解析"],"content":"tornado.iostream.BaseIOStream._find_read_pos() def _find_read_pos(self): # 试图在读取缓冲区中找到满足当前待读取的位置 # 如果能够满足当前读取，则返回缓冲区中的位置;如果不能，则返回None。 if (self._read_bytes is not None and (self._read_buffer_size \u003e= self._read_bytes or (self._read_partial and self._read_buffer_size \u003e 0))): num_bytes = min(self._read_bytes, self._read_buffer_size) return num_bytes elif self._read_delimiter is not None: # Multi-byte delimiters (e.g. '\\r\\n') may straddle two # chunks in the read buffer, so we can't easily find them # without collapsing the buffer. However, since protocols # using delimited reads (as opposed to reads of a known # length) tend to be \"line\" oriented, the delimiter is likely # to be in the first few chunks. Merge the buffer gradually # since large merges are relatively expensive and get undone in # _consume(). if self._read_buffer: loc = self._read_buffer.find(self._read_delimiter, self._read_buffer_pos) if loc != -1: loc -= self._read_buffer_pos delimiter_len = len(self._read_delimiter) self._check_max_bytes(self._read_delimiter, loc + delimiter_len) return loc + delimiter_len self._check_max_bytes(self._read_delimiter, self._read_buffer_size) elif self._read_regex is not None: # self._read_buffer在_read_to_buffer方法中定义 if self._read_buffer: # 在已读取的数据中从上一次的位置开始匹配正则，self._read_buffer_pos在 # _consume方法中定义 m = self._read_regex.search(self._read_buffer, self._read_buffer_pos) # 匹配成功 if m is not None: # 获取本次读取的数据大小 loc = m.end() - self._read_buffer_pos # 判断本次读取的数据是否超过了最大读取数据量，如果是则 # 报错UnsatisfiableReadError self._check_max_bytes(self._read_regex, loc) return loc # 如果没有匹配到正则表达式，则检查读取的数据量是否超过了最大读取数据量 # self._read_buffer_size在_read_to_buffer方法中定义 self._check_max_bytes(self._read_regex, self._read_buffer_size) return None 在 read_until_regex() 方法中，self._read_regex 是存在的，self._read_delimiter 是针对 read_until() 方法的操作。在此方法中可知，传入 read_until_regex() 的 max_bytes 必须要大于 self.read_chunk_size（socket.recv 每次循环接收的数据量），否则会直接报错。 ","date":"2017-11-06","objectID":"/posts/tornado_iostream/:1:4","tags":["Tornado"],"title":"tornado iostream 实现解析","uri":"/posts/tornado_iostream/"},{"categories":["Tornado 源码解析"],"content":"tornado.iostream.BaseIOStream._read_to_buffer_loop() def _read_to_buffer_loop(self): # This method is called from _handle_read and _try_inline_read. try: # 根据不同的方法调用获取对应的目标数据量 if self._read_bytes is not None: target_bytes = self._read_bytes elif self._read_max_bytes is not None: target_bytes = self._read_max_bytes elif self.reading(): # 对于没有max_bytes参数的read_until或者read_until_close，应在扫描分 # 隔符之前尽可能多地进行读取。 target_bytes = None else: target_bytes = 0 next_find_pos = 0 # 假装有一个挂起的回调，以便_read_to_buffer中的EOF不会触发立即关闭回调。 # 在这个方法（_try_inline_read）的最后，我们要么通过_read_from_buffer # 建立一个真正的等待回调，要么运行关闭回调。避免程序中途退出。因为 # 在_maybe_run_close_callback、_maybe_add_error_listener等方法中都 # 会比较self._pending_callbacks参数值，如果该值不为0，则表示该loop过程 # 还在执行，不能去运行关闭回调。最后的finally块中会递减该值。 self._pending_callbacks += 1 while not self.closed(): # 从socket中读数据，直到得到EWOULDBLOCK（当一个非阻塞的操作没有数据 # 操作时，如读操作时，缓存区空了，此时没有数据读了，写操作时，缓存区已 # 满，无法再写入）或者类似的错误。在Windows上为EWOULDBLOCK，Linux # 上为EAGAIN if self._read_to_buffer() == 0: break self._run_streaming_callback() # 如果已经读完了所有可以使用的字节，就跳出这个循环。不能在 # 这里调用read_from_buffer，因为它与pending_callback和 # error_listener机制的微妙交互。 # 如果已经达到target_bytes，则已经读取完成了。 if (target_bytes is not None and self._read_buffer_size \u003e= target_bytes): break # 否则，需要调用更加昂贵的find_read_pos。 在每次读取时这样做 # 效率不高，所以在第一次读取以及每当读取缓冲区大小加倍时都要这样做。 if self._read_buffer_size \u003e= next_find_pos: pos = self._find_read_pos() if pos is not None: return pos next_find_pos = self._read_buffer_size * 2 return self._find_read_pos() finally: # 递减该属性值，以便能执行关闭回调等操作 self._pending_callbacks -= 1 读取底层 socket fd 中的数据，将其保存到缓存中。通过判断该 socket 连接是否断开进行循环读取操作，最主要的就是 self._read_to_buffer() 方法，封装了对 socket 中读取到的数据处理过程。最终会调用 socket 原生的读取函数 socket.recv(buffer)，即 read_from_fd() 函数，此函数在 IOStream 中被实现。 ","date":"2017-11-06","objectID":"/posts/tornado_iostream/:1:5","tags":["Tornado"],"title":"tornado iostream 实现解析","uri":"/posts/tornado_iostream/"},{"categories":["Tornado 源码解析"],"content":"tornado.iostream.BaseIOStream._read_from_buffer() def _read_from_buffer(self, pos): # 尝试从缓冲区中完成当前正在等待的读取 # 重置参数 self._read_bytes = self._read_delimiter = self._read_regex = None self._read_partial = False self._run_read_callback(pos, False) ","date":"2017-11-06","objectID":"/posts/tornado_iostream/:1:6","tags":["Tornado"],"title":"tornado iostream 实现解析","uri":"/posts/tornado_iostream/"},{"categories":["Tornado 源码解析"],"content":"tornado.iostream.BaseIOStream._run_read_callback() def _run_read_callback(self, size, streaming): # 判断是否要运行stream回调函数，read_until_regex没有该函数 if streaming: callback = self._streaming_callback else: callback = self._read_callback self._read_callback = self._streaming_callback = None # 如果self._read_future不为空 if self._read_future is not None: assert callback is None future = self._read_future self._read_future = None future.set_result(self._consume(size)) if callback is not None: assert (self._read_future is None) or streaming self._run_callback(callback, self._consume(size)) else: # If we scheduled a callback, we will add the error listener # afterwards. If we didn't, we have to do it now. self._maybe_add_error_listener() 首先通过 streaming 参数，决定 callback 是何值。streaming 参数针对 read_bytes、read_until_close 这两个读取方法，而 read_until、read_until_regex 没有 streaming_callback。针对没有 streaming_callback 的方法，会判断 self._read_callback 和 self._read_future，如果 self._read_future 不会空，则会使用 Future 对象将数据返回，否则通过 callback 回调返回。Future 详解参考：tornado_concurrent ","date":"2017-11-06","objectID":"/posts/tornado_iostream/:1:7","tags":["Tornado"],"title":"tornado iostream 实现解析","uri":"/posts/tornado_iostream/"},{"categories":["Tornado 源码解析"],"content":"tornado.iostream.BaseIOStream._consume() def _consume(self, loc): # 消耗缓存区的loc数量数据，并将其返回 if loc == 0: return b\"\" assert loc \u003c= self._read_buffer_size # 获取已读取到缓存区的总数据self._read_buffer，并截取其中从位置 # self._read_buffer_pos（最开始为0）到self._read_buffer_pos + loc # 中的数据 b = (memoryview(self._read_buffer) [self._read_buffer_pos:self._read_buffer_pos + loc] ).tobytes() self._read_buffer_pos += loc self._read_buffer_size -= loc # Amortized O(1) shrink # (this heuristic is implemented natively in Python 3.4+ # but is replicated here for Python 2) if self._read_buffer_pos \u003e self._read_buffer_size: del self._read_buffer[:self._read_buffer_pos] self._read_buffer_pos = 0 return b ","date":"2017-11-06","objectID":"/posts/tornado_iostream/:1:8","tags":["Tornado"],"title":"tornado iostream 实现解析","uri":"/posts/tornado_iostream/"},{"categories":["Tornado 源码解析"],"content":"tornado http1connection 解析 tornado http1connection 主要是对 http 协议进行了封装。 ","date":"2017-11-06","objectID":"/posts/tornado_http1connection/:1:0","tags":["Tornado"],"title":"tornado http1connection 实现解析","uri":"/posts/tornado_http1connection/"},{"categories":["Tornado 源码解析"],"content":"tornado.http1connection.HTTP1ServerConnection.start_serving() def start_serving(self, delegate): # 这里断言delegate是httputil.HTTPServerConnectionDelegate的实例 assert isinstance(delegate, httputil.HTTPServerConnectionDelegate) self._serving_future = self._server_request_loop(delegate) self.stream.io_loop.add_future(self._serving_future, lambda f: f.result()) 开始处理这个连接上的请求，在 tornado.HTTPServer.handle_stream() 中调用此方法，传入的参数为 HTTPServer 的实例，即 delegate 为 HTTPServer 实例，由于 HTTPServer 继承至 httputil.HTTPServerConnectionDelegate，所以断言成功，程序开始执行 self._server_request_loop()。 ","date":"2017-11-06","objectID":"/posts/tornado_http1connection/:1:1","tags":["Tornado"],"title":"tornado http1connection 实现解析","uri":"/posts/tornado_http1connection/"},{"categories":["Tornado 源码解析"],"content":"tornado.http1connection.HTTP1ServerConnection._server_request_loop() @gen.coroutine def _server_request_loop(self, delegate): try: while True: # 初始化HTTP1Connection实例 conn = HTTP1Connection(self.stream, False, self.params, self.context) # 调用delegate的start_request处理连接请求 request_delegate = delegate.start_request(self, conn) try: # 读取http响应 ret = yield conn.read_response(request_delegate) except (iostream.StreamClosedError, iostream.UnsatisfiableReadError): return except _QuietException: # This exception was already logged. conn.close() return except Exception: gen_log.error(\"Uncaught exception\", exc_info=True) conn.close() return # 如果ret为false，则表示读取了完整的http响应 if not ret: return yield gen.moment finally: delegate.on_close(self)v 方法中调用了 delegate.start_request(self, conn)，即 tornado.httpserver.HTTPServer.start_request()，得到一个 httputil.HTTPMessageDelegate 实例，即 request_delegate。接下来开始读取响应数据。 ","date":"2017-11-06","objectID":"/posts/tornado_http1connection/:1:2","tags":["Tornado"],"title":"tornado http1connection 实现解析","uri":"/posts/tornado_http1connection/"},{"categories":["Tornado 源码解析"],"content":"tornado.http1connection.HTTP1Connection.read_response() def read_response(self, delegate): # 判断是否需要解压缩数据 if self.params.decompress: delegate = _GzipMessageDelegate(delegate, self.params.chunk_size) return self._read_message(delegate) ","date":"2017-11-06","objectID":"/posts/tornado_http1connection/:1:3","tags":["Tornado"],"title":"tornado http1connection 实现解析","uri":"/posts/tornado_http1connection/"},{"categories":["Tornado 源码解析"],"content":"tornado.http1connection.HTTP1Connection._read_message() @gen.coroutine def _read_message(self, delegate): need_delegate_close = False try: # 读取请求header数据，返回Future对象 header_future = self.stream.read_until_regex( b\"\\r?\\n\\r?\\n\", max_bytes=self.params.max_header_size) # 判断header_timeout是否为None，如果为None，则直接读取header_future数据 if self.params.header_timeout is None: header_data = yield header_future else: try: # header超时实现 header_data = yield gen.with_timeout( self.stream.io_loop.time() + self.params.header_timeout, header_future, io_loop=self.stream.io_loop, quiet_exceptions=iostream.StreamClosedError) except gen.TimeoutError: self.close() raise gen.Return(False) # 解析header信息，HTTP协议分为request-line、request-header、 # request-body三个部分的。在查看各浏览器、服务器配置的时候往往将 # request-line、request-header归为一类了 start_line, headers = self._parse_headers(header_data) # 判断是否作为客户端 if self.is_client: # 如果作为客户端，则解析服务端响应起始行 start_line = httputil.parse_response_start_line(start_line) self._response_start_line = start_line else: # 如果作为服务端，则解析客户端请求起始行 start_line = httputil.parse_request_start_line(start_line) self._request_start_line = start_line self._request_headers = headers # 判断是否能保持长链接，即Connection: keep_alive self._disconnect_on_finish = not self._can_keep_alive( start_line, headers) need_delegate_close = True with _ExceptionLoggingContext(app_log): header_future = delegate.headers_received(start_line, headers) if header_future is not None: yield header_future if self.stream is None: # We've been detached. need_delegate_close = False raise gen.Return(False) skip_body = False if self.is_client: if (self._request_start_line is not None and self._request_start_line.method == 'HEAD'): skip_body = True code = start_line.code if code == 304: # 304 responses may include the content-length header # but do not actually have a body. # http://tools.ietf.org/html/rfc7230#section-3.3 skip_body = True if code \u003e= 100 and code \u003c 200: # 1xx responses should never indicate the presence of # a body. if ('Content-Length' in headers or 'Transfer-Encoding' in headers): raise httputil.HTTPInputError( \"Response code %d cannot have body\" % code) # TODO: client delegates will get headers_received twice # in the case of a 100-continue. Document or change? yield self._read_message(delegate) else: if (headers.get(\"Expect\") == \"100-continue\" and not self._write_finished): self.stream.write(b\"HTTP/1.1 100 (Continue)\\r\\n\\r\\n\") if not skip_body: body_future = self._read_body( start_line.code if self.is_client else 0, headers, delegate) if body_future is not None: if self._body_timeout is None: yield body_future else: try: yield gen.with_timeout( self.stream.io_loop.time() + self._body_timeout, body_future, self.stream.io_loop, quiet_exceptions=iostream.StreamClosedError) except gen.TimeoutError: gen_log.info(\"Timeout reading body from %s\", self.context) self.stream.close() raise gen.Return(False) self._read_finished = True if not self._write_finished or self.is_client: need_delegate_close = False with _ExceptionLoggingContext(app_log): delegate.finish() # If we're waiting for the application to produce an asynchronous # response, and we're not detached, register a close callback # on the stream (we didn't need one while we were reading) if (not self._finish_future.done() and self.stream is not None and not self.stream.closed()): self.stream.set_close_callback(self._on_connection_close) yield self._finish_future if self.is_client and self._disconnect_on_finish: self.close() if self.stream is None: raise gen.Return(False) except httputil.HTTPInputError as e: gen_log.info(\"Malformed HTTP message from %s: %s\", self.context, e) self.close() raise gen.Return(False) finally: if need_delegate_close: with _ExceptionLoggingContext(app_log): delegate.on_connection_close() header_future = None self._clear_callbacks() raise gen.Return(True) 重要方法，首先会调用 stream 的 read_until_regex 方法开始读取客户端请求传入的头数据（http 协议的header），返回一个 Future 对象，即 header_future，详解参考：tornado_iostream#read_until_regex()；接着会判断是否有设置 header_timeout，而在该方","date":"2017-11-06","objectID":"/posts/tornado_http1connection/:1:4","tags":["Tornado"],"title":"tornado http1connection 实现解析","uri":"/posts/tornado_http1connection/"},{"categories":["Tornado 源码解析"],"content":"tornado netutil 实现解析 ","date":"2017-09-25","objectID":"/posts/tornado_netutil/:1:0","tags":["Tornado"],"title":"tornado netutil 实现解析","uri":"/posts/tornado_netutil/"},{"categories":["Tornado 源码解析"],"content":"tornado.netutil.bind_sockets() 该方法创建绑定到给定端口和地址的监听套接字 socket。返回套接字对象的列表，比如给定的 address 参数映射到多个 IP 地址，则返回多个 socket，最常见的是混合使用 IPv4 与 IPv6，则会创建对应的两个 socket。 def bind_sockets(port, address=None, family=socket.AF_UNSPEC, backlog=_DEFAULT_BACKLOG, flags=None, reuse_port=False): # 当设置了端口复用时，会检查系统是否支持端口复用功能 if reuse_port and not hasattr(socket, \"SO_REUSEPORT\"): raise ValueError(\"the platform doesn't support SO_REUSEPORT\") sockets = [] if address == \"\": address = None # 如果系统不支持IPv6并且参数family为AF_UNSPEC，则family选择IPv4协议 if not socket.has_ipv6 and family == socket.AF_UNSPEC: family = socket.AF_INET if flags is None: flags = socket.AI_PASSIVE bound_port = None # 循环遍历获取的地址信息 for res in set(socket.getaddrinfo(address, port, family, socket.SOCK_STREAM, 0, flags)): af, socktype, proto, canonname, sockaddr = res #　排除另类数据 if (sys.platform == 'darwin' and address == 'localhost' and af == socket.AF_INET6 and sockaddr[3] != 0): continue try: # 创建socket对象 sock = socket.socket(af, socktype, proto) except socket.error as e: if errno_from_exception(e) == errno.EAFNOSUPPORT: continue raise # 设置close-on-exec标志位 set_close_exec(sock.fileno()) # 设置SO_REUSEADDR if os.name != 'nt': sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #　设置SO_REUSEPORT if reuse_port: sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1) if af == socket.AF_INET6: # 在linux上，ipv6 socket也默认接受ipv4，但是这样就无法绑定到ipv4 # 中的0.0.0.0和ipv6中的::。 # 在其他系统上，单独的套接字必须用于监听ipv4和ipv6。 # 为了保持一致性，请务必在ipv6套接字上禁用ipv4，并在需要时使用 # 单独的ipv4套接字。 # Windows上的Python 2.x没有IPPROTO_IPV6。 if hasattr(socket, \"IPPROTO_IPV6\"): sock.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 1) # 当port参数为None时，会自动分配绑定端口，该端口应同时绑定在IPv4和IPv6。 # 当第一次循环时，bound_port会记录系统自动分配的port，然后应用到接下来的循环 host, requested_port = sockaddr[:2] if requested_port == 0 and bound_port is not None: sockaddr = tuple([host, bound_port] + list(sockaddr[2:])) # 设置socket为非阻塞 sock.setblocking(0) # 绑定socket sock.bind(sockaddr) # 记录下本次绑定的端口 bound_port = sock.getsockname()[1] # 开启socket监听 sock.listen(backlog) sockets.append(sock) return sockets 如上基本为 socket 的常规操作，主要操作放在了 IPv4 与 IPv6 的兼容方面。最终返回分别基于 IPv4 与 IPv6 两个 socket 对象组成的 list。 ","date":"2017-09-25","objectID":"/posts/tornado_netutil/:1:1","tags":["Tornado"],"title":"tornado netutil 实现解析","uri":"/posts/tornado_netutil/"},{"categories":["Tornado 源码解析"],"content":"tornado.netutil.add_accept_handler() 正是该方法将 web 服务器与 IOLoop 连接了起来，主要用于添加一个 IOLoop 事件处理器来接受服务器 socket 上的新连接（来自客户端的连接）。当一个客户端连接被 accept，callback 函数将会被调用。 def add_accept_handler(sock, callback, io_loop=None): if io_loop is None: io_loop = IOLoop.current() def accept_handler(fd, events): # 在我们处理回调时可能会有更多的连接; 为了防止其他任务的饥饿（如果该值过大， # 可能会导致程序执行当前队列时间过长，而后续任务无法被执行），我们必须限制 # 我们一次接受的连接数。理想情况下，我们将接受输入此方法时等待的连接数， # 但是此信息不可用（而且在运行任何回调之前重新排列此方法以调用accept() # 多次可能会对多进程配置中的负载平衡产生不利影响）。 相反，我们使用（默认） # listen backlog作为我们可以合理接受的连接数的粗略启发式。 for i in xrange(_DEFAULT_BACKLOG): try: connection, address = sock.accept() except socket.error as e: # _ERRNO_WOULDBLOCK表示我们接受了每个有用的连接 if errno_from_exception(e) in _ERRNO_WOULDBLOCK: return # ECONNABORTED表示有个链接已经被关闭了但任然在接受队列中 if errno_from_exception(e) == errno.ECONNABORTED: continue raise # 调用callback callback(connection, address) # 将服务器端sock以读事件注册到epoll，即epoll一直监听着服务器端socket，只要有 # 客户端连接到服务器端socket，epoll就会触发，epoll.poll()方法就会返回，IOLoop # 就会调用accept_handler方法，最终调用callback(connection, address) io_loop.add_handler(sock, accept_handler, IOLoop.READ) ","date":"2017-09-25","objectID":"/posts/tornado_netutil/:1:2","tags":["Tornado"],"title":"tornado netutil 实现解析","uri":"/posts/tornado_netutil/"},{"categories":["Tornado 源码解析"],"content":"tornado httpserver 详解 tornado httpserver 封装了对 http 请求的处理，是一个非阻塞、单线程的 HTTP 服务器。一个完整的 web 服务器示例如下，启动脚本后在浏览器访问 http://localhost:8888，页面会显示 Hello, world： import tornado.httpserver import tornado.ioloop import tornado.options import tornado.web from tornado.options import define, options define(\"port\", default=8888, help=\"run on the given port\", type=int) class MainHandler(tornado.web.RequestHandler): def get(self): self.write(\"Hello, world\") def main(): # 解析启动命令 tornado.options.parse_command_line() # 创建Application示例 application = tornado.web.Application([ (r\"/\", MainHandler), ]) # 创建HTTPServer实例 http_server = tornado.httpserver.HTTPServer(application) # 监听端口，创建服务器socket http_server.listen(options.port) # 获取IOLoop并启动 tornado.ioloop.IOLoop.current().start() if __name__ == \"__main__\": main() 从实例中可以看到一个 web 服务器同时使用了 tornado.httpserver.HTTPServer 与 tornado.ioloop.IOLoop，二者不可分离。 HTTPServer 有三个父类：tornado.tcpserver.TCPServer、tornado.util.Configurable、 tornado.httputil.HTTPServerConnectionDelegate 当创建 HTTPServer 实例，即调用 tornado.httpserver.HTTPServer(application) 时，程序首先会调用 tornado.util.Configurable.__new__() 方法创建 HTTPServer 实例，对 tornado.util.Configurable 的详解可参考： tornado 配置类 Configurable，实例创建完成之后，会调用 tornado.httpserver.HTTPServer.initialize() 初始化参数配置。 ","date":"2017-09-08","objectID":"/posts/tornado_httpserver/:1:0","tags":["Tornado"],"title":"tornado httpserver 实现解析","uri":"/posts/tornado_httpserver/"},{"categories":["Tornado 源码解析"],"content":"tornado.httpserver.HTTPServer.initialize() def initialize(self, request_callback, no_keep_alive=False, io_loop=None, xheaders=False, ssl_options=None, protocol=None, decompress_request=False, chunk_size=None, max_header_size=None, idle_connection_timeout=None, body_timeout=None, max_body_size=None, max_buffer_size=None, trusted_downstream=None): # 由tornado.util.Configurable分析可知，self.request_callback为 # tornado.web.Application实例 self.request_callback = request_callback self.no_keep_alive = no_keep_alive self.xheaders = xheaders self.protocol = protocol self.conn_params = HTTP1ConnectionParameters( decompress=decompress_request, chunk_size=chunk_size, max_header_size=max_header_size, header_timeout=idle_connection_timeout or 3600, max_body_size=max_body_size, body_timeout=body_timeout, no_keep_alive=no_keep_alive) # 调用TCPServer的初始化方法 TCPServer.__init__(self, io_loop=io_loop, ssl_options=ssl_options, max_buffer_size=max_buffer_size, read_chunk_size=chunk_size) self._connections = set() self.trusted_downstream = trusted_downstream 在开始的示例中实例化 HTTPServer 之后会执行：http_server.listen(options.port)，该代码会创建服务器 socket，监听 8888 端口，socket/tcp 详解可参考： socket/tcp，listen() 方法在 tornado.tcpserver.TCPServer 中实现。 ","date":"2017-09-08","objectID":"/posts/tornado_httpserver/:1:1","tags":["Tornado"],"title":"tornado httpserver 实现解析","uri":"/posts/tornado_httpserver/"},{"categories":["Tornado 源码解析"],"content":"tornado.tcpserver.TCPServer.listen() def listen(self, port, address=\"\"): # 调用bind_sockets sockets = bind_sockets(port, address=address) self.add_sockets(sockets) listen() 方法中就两行代码，分别调用了两个方法：tornado.netutil.bind_sockets()、add_sockets。tornado.netutil.bind_sockets() 详解参考：tornado_netutil#bind_sockets() ","date":"2017-09-08","objectID":"/posts/tornado_httpserver/:1:2","tags":["Tornado"],"title":"tornado httpserver 实现解析","uri":"/posts/tornado_httpserver/"},{"categories":["Tornado 源码解析"],"content":"tornado.tcpserver.TCPServer.add_sockets() def add_sockets(self, sockets): # 获取当前IOLoop对象，此时还没有start if self.io_loop is None: self.io_loop = IOLoop.current() for sock in sockets: # 保存socket到self._sockets self._sockets[sock.fileno()] = sock # 重点方法 add_accept_handler(sock, self._handle_connection, io_loop=self.io_loop) add_sockets() 方法最重要的是调用 add_accept_handler() 函数，详解参考：tornado_netutil#add_accept_handler()，从 add_accept_handler() 函数的详解可知，客户端连接到服务器之后的数据传输，最终调用 self._handle_connection() 方法完成。 ","date":"2017-09-08","objectID":"/posts/tornado_httpserver/:1:3","tags":["Tornado"],"title":"tornado httpserver 实现解析","uri":"/posts/tornado_httpserver/"},{"categories":["Tornado 源码解析"],"content":"tornado.tcpserver.TCPServer._handle_connection() def _handle_connection(self, connection, address): # 对ssl相关处理，可以略过 if self.ssl_options is not None: assert ssl, \"Python 2.6+ and OpenSSL required for SSL\" try: connection = ssl_wrap_socket(connection, self.ssl_options, server_side=True, do_handshake_on_connect=False) except ssl.SSLError as err: if err.args[0] == ssl.SSL_ERROR_EOF: return connection.close() else: raise except socket.error as err: if errno_from_exception(err) in (errno.ECONNABORTED, errno.EINVAL): return connection.close() else: raise try: # 如果是ssl连接，则使用SSLIOStream处理connection，否则使用IOStream if self.ssl_options is not None: stream = SSLIOStream(connection, io_loop=self.io_loop, max_buffer_size=self.max_buffer_size, read_chunk_size=self.read_chunk_size) else: stream = IOStream(connection, io_loop=self.io_loop, max_buffer_size=self.max_buffer_size, read_chunk_size=self.read_chunk_size) # 处理stream future = self.handle_stream(stream, address) if future is not None: # 将future添加到IOLoop self.io_loop.add_future(gen.convert_yielded(future), lambda f: f.result()) except Exception: app_log.error(\"Error in connection callback\", exc_info=True) 以非 ssl 请求为例，tornado 会实例化 tornado.iostream.IOStream 对象，用它去处理流，该对象主要封装了对请求数据读写的一些操作。之后会调用 self.handle_stream(stream, address)，而该方法在 tornado.HTTPServer中被实现。 ","date":"2017-09-08","objectID":"/posts/tornado_httpserver/:1:4","tags":["Tornado"],"title":"tornado httpserver 实现解析","uri":"/posts/tornado_httpserver/"},{"categories":["Tornado 源码解析"],"content":"tornado.httpserver.HTTPServer.handle_stream() def handle_stream(self, stream, address): # 将相关参数保存上下文中，以便之后获取 context = _HTTPRequestContext(stream, address, self.protocol, self.trusted_downstream) # 初始化HTTP1ServerConnection实例 conn = HTTP1ServerConnection( stream, self.conn_params, context) # 保存连接conn到self._connections self._connections.add(conn) # 开始在该conn连接上处理请求 conn.start_serving(self) 该方法主要是完成了对 HTTP1ServerConnection 的初始化，以及通过调用 start_serving 开始处理请求。http1connection.HTTP1ServerConnection.start_serving() 详解参考：tornado_http1connection#start_serving ","date":"2017-09-08","objectID":"/posts/tornado_httpserver/:1:5","tags":["Tornado"],"title":"tornado httpserver 实现解析","uri":"/posts/tornado_httpserver/"},{"categories":["Tornado 源码解析"],"content":"相关方法解析 ","date":"2017-09-08","objectID":"/posts/tornado_httpserver/:2:0","tags":["Tornado"],"title":"tornado httpserver 实现解析","uri":"/posts/tornado_httpserver/"},{"categories":["Tornado 源码解析"],"content":"tornado.httpserver.HTTPServer.start_request() def start_request(self, server_conn, request_conn): if isinstance(self.request_callback, httputil.HTTPServerConnectionDelegate): delegate = self.request_callback.start_request(server_conn, request_conn) else: delegate = _CallableAdapter(self.request_callback, request_conn) if self.xheaders: delegate = _ProxyAdapter(delegate, request_conn) return delegate 方法实现了父类 tornado.httputil.HTTPServerConnectionDelegate 中的 start_request() 方法，当新的请求开始时，这个方法会被服务器调用。 通过上面 tornado.httpserver.HTTPServer.initialize() 详解可知，self.request_callback 为 tornado.web.Application 实例，而 tornado.web.Application 刚好继承至httputil.HTTPServerConnectionDelegate，则会调用 tornado.web.Application 的 start_request() 方法。 最终返回的是继承至 httputil.HTTPMessageDelegate 的 _RoutingDelegate 对象，即 delegate 为 httputil.HTTPMessageDelegate 实例。 ","date":"2017-09-08","objectID":"/posts/tornado_httpserver/:2:1","tags":["Tornado"],"title":"tornado httpserver 实现解析","uri":"/posts/tornado_httpserver/"},{"categories":["Tornado 源码解析"],"content":"tornado posix 实现解析 tornado.platform.posix，Posix 平台特定功能的实现。Posix 平台指“可移植操作系统接口”（Portable Operation System Interface），最后的 x 代表类 Unix 系统。该文件主要是对 Linux 系统相关 API 的实现，Windows NT 系列实现为 tornado.platform.windows、tornado.platform.common。 ","date":"2017-09-01","objectID":"/posts/tornado_platform_posix/:1:0","tags":["Tornado"],"title":"tornado posix 实现解析","uri":"/posts/tornado_platform_posix/"},{"categories":["Tornado 源码解析"],"content":"close-on-exec 标志详解 当我们 fork 子进程时，子进程以写时复制（COW，Copy-On-Write）方式获得父进程的数据空间、堆和栈副本，这其中也包括文件描述符。刚刚 fork 成功时，父子进程中相同的文件描述符指向系统文件表中的同一项（这也意味着他们共享同一文件偏移量）。 fork 接着，在子进程中我们会 exec 另一个程序，此时会用全新的程序替换子进程的正文，数据，堆和栈等。此时保存文件描述符的变量当然也不存在了，我们就无法关闭无用的文件描述符了。所以通常我们会 fork 子进程后在子进程中直接执行 close 关掉无用的文件描述符，然后再执行 exec。 但是在复杂系统中，有时我们 fork 子进程时已经不知道打开了多少个文件描述符（包括 socket 句柄等），这此时进行逐一清理确实有很大难度。我们期望的是能在 fork 子进程前打开某个文件句柄时就指定好：“这个句柄我在 fork 子进程后执行 exec 时就关闭”。其实是有这样的方法的：即所谓的 close-on-exec。设置 close-on-exec 为 FD_CLOEXEC，即 1（系统默认为 0），这样，当 fork 子进程后，仍然可以使用 fd。但执行 exec 后系统就会字段关闭子进程中的 fd 了，即不能再在该文件中读写数据了。 ","date":"2017-09-01","objectID":"/posts/tornado_platform_posix/:1:1","tags":["Tornado"],"title":"tornado posix 实现解析","uri":"/posts/tornado_platform_posix/"},{"categories":["Tornado 源码解析"],"content":"tornado.platform.posix def set_close_exec(fd): # 取得与文件描述符fd联合的close-on-exec标志，类似FD_CLOEXEC。 # 如果返回值和FD_CLOEXEC进行与运算结果是0的话，文件保持交叉式访问exec()， # 否则如果通过exec运行的话，文件将被关闭 flags = fcntl.fcntl(fd, fcntl.F_GETFD) # 设置close-on-exec标志，该标志以参数arg的FD_CLOEXEC位决定， # 很多现存的涉及文件描述符标志的程序并不使用常数 FD_CLOEXEC， # 而是将此标志设置为0(系统默认，在exec时不关闭)或1(在exec时关闭) fcntl.fcntl(fd, fcntl.F_SETFD, flags | fcntl.FD_CLOEXEC) def _set_nonblocking(fd): # 取得fd的文件状态标志 flags = fcntl.fcntl(fd, fcntl.F_GETFL) # 设置fd描述符状态标志为非阻塞，如果read(1024)调用没有可读取的数据， # 或者如果write(1024)操作时写缓存区已满， # 则read或write调用将返回-1和EAGAIN错误，而不会被阻塞 fcntl.fcntl(fd, fcntl.F_SETFL, flags | os.O_NONBLOCK) # 唤醒者（另一线程） class Waker(interface.Waker): def __init__(self): r, w = os.pipe() _set_nonblocking(r) _set_nonblocking(w) set_close_exec(r) set_close_exec(w) self.reader = os.fdopen(r, \"rb\", 0) self.writer = os.fdopen(w, \"wb\", 0) def fileno(self): return self.reader.fileno() def write_fileno(self): return self.writer.fileno() def wake(self): try: self.writer.write(b\"x\") except (IOError, ValueError): pass def consume(self): try: while True: result = self.reader.read() if not result: break except IOError: pass def close(self): self.reader.close() common.try_close(self.writer) 通过上面对 close-on-exec 标志的详解，再解读 tornado.platform.posix 就容易理解了。set_close_exec(fd) 就是为了设置 close-on-exec 标志位为 1；_set_nonblocking(fd) 为了设置 IO 读写为非阻塞模式。 类 Waker 可以解释为唤醒者。它继承至 tornado.platform.interface.Waker，是一个类似 socket（pipe 管道）的对象，可以从 select.select() 或 epoll.poll() 等类似函数唤醒另一个线程。tornado.ioloop.IOLoop 将会把Waker的读文件描述符添加到 select（或 epoll 或 kqueue）中。 由于 epoll.poll()（select.select()）函数是阻塞的，即当没有读写事件发生时会休眠，而当另一个线程想要唤醒 IOLoop 时，它会调用 Waker.wake()，向 pipe 中写入数据，此时，已经被注册到 epoll 中的读管道 pipe 会被触发，从而 epoll.poll() 函数返回，即所谓唤醒了 IOLoop。 IOLoop 一旦醒来，它将调用 Waker.consume() 回调函数，以进行必要的每次唤醒清理，即将为唤醒 IOLoop 而写入的数据读完。当 IOLoop 关闭时，它也关闭了它的 waker。 ","date":"2017-09-01","objectID":"/posts/tornado_platform_posix/:1:2","tags":["Tornado"],"title":"tornado posix 实现解析","uri":"/posts/tornado_platform_posix/"},{"categories":["Tornado 源码解析"],"content":"tornado util configurable 实现解析 tornado.util.Configurable，一个配置类，是工厂模式的实现，通过使用构造函数（__new__()）作为工厂方法。其子类必须实现 configurable_base()、configurable_default()、initialize()。通过调用 configure() 函数去配置当基类（不是指 Configurable，而是继承至 Configurable 的类，如 tornado.ioloop.IOLoop）被实例化时使用的实现类，以及配置其实现类初始化的关键字参数。 示例 from tornado import httpclient httpclient.AsyncHTTPClient.configure (\"tornado.curl_httpclient.CurlAsyncHTTPClient\", max_clients=10000) http_client = httpclient.AsyncHTTPClient() 以 tornado.httpclient.AsyncHTTPClient 为示例来开始 tornado.util.Configurable 的剖析。从 tornado 源码可知，AsyncHTTPClient 继承至 Configurable，同时 tornado.curl_httpclient.CurlAsyncHTTPClient 继承至 AsyncHTTPClient。 第二行 AsyncHTTPClient 调用 configure 去设置它的实现类及关键字参数 max_clients=10000。其源码中直接调用了父类（Configurable）的 configure() 函数。 tornado.httpclient.AsyncHTTPClient.configure() @classmethod def configure(cls, impl, **kwargs): super(AsyncHTTPClient, cls).configure(impl, **kwargs) tornado.util.Configurable.configure() @classmethod def configure(cls, impl, **kwargs): # cls为AsyncHTTPClient，获取可配置层次结构的基类base（AsyncHTTPClient） base = cls.configurable_base() # 由上面的例子得：impl=\"tornado.curl_httpclient.CurlAsyncHTTPClient\" if isinstance(impl, (str, unicode_type)): # 引入tornado.curl_httpclient.CurlAsyncHTTPClient到当前上下文环境 impl = import_object(impl) if impl is not None and not issubclass(impl, cls): raise ValueError(\"Invalid subclass of %s\" % cls) # 通过全局变量保存数据，这两个变量是初始化实例 # （tornado.util.Configurable.__new__()）时非常重要的数据 # 值为：tornado.curl_httpclient.CurlAsyncHTTPClient base.__impl_class = impl # 值为：{\"max_clients\": 10000} base.__impl_kwargs = kwargs 第三行获取 AsyncHTTPClient 实例，将会调用tornado.util.Configurable.__new__()函数。 tornado.util.Configurable.new() def __new__(cls, *args, **kwargs): # cls为AsyncHTTPClient，获取可配置层次结构的基类， # 通常是其自身（如tornado.httpclient.AsyncHTTPClient.configurable_base()） base = cls.configurable_base() init_kwargs = {} # 判断cls是否是基类base if cls is base: # 获取当前配置的实现类，因为之前配置过实现类，即第二行， # 所以得到impl为tornado.curl_httpclient.CurlAsyncHTTPClient impl = cls.configured_class() # 判断configure()函数配置的关键字参数是否为空 if base.__impl_kwargs: # 更新初始化参数字典，因为之前配置过关键字参数，即第二行， # base.__impl_kwargs={\"max_clients\": 10000} init_kwargs.update(base.__impl_kwargs) else: # 实现类即为cls impl = cls # 更新初始化参数字典 init_kwargs.update(kwargs) # 实例化cls，如示例，instance为tornado.curl_httpclient.CurlAsyncHTTPClient instance = super(Configurable, cls).__new__(impl) # 初始化实例参数 instance.initialize(*args, **init_kwargs) return instance tornado.util.Configurable.configured_class() @classmethod def configured_class(cls): # cls为AsyncHTTPClient base = cls.configurable_base() # 判断有没有调用tornado.util.Configurable.configure()函数进行配置， # 如果没有配置过，就调用默认设置configurable_default() if cls.__impl_class is None: base.__impl_class = cls.configurable_default() return base.__impl_class tornado.util.Configurable.configured_class() 函数是选取实现类的关键，它会判断是否调用过 tornado.util.Configurable.configure() 函数去配置实现类了，然后以此选择相应的实现类。 ","date":"2017-08-31","objectID":"/posts/tornado_util_configurable/:1:0","tags":["Tornado"],"title":"tornado util configurable 实现解析","uri":"/posts/tornado_util_configurable/"},{"categories":["Tornado 源码解析"],"content":"tornado ioloop 实现解析 tornado ioloop 是一个基于水平触发的非阻塞 socket 的 IO 事件循环。在 Linux 系统上会使用 epoll，Mac 和 BSD 系统中使用 kqueue，否则使用 select。在分析源码之前需要搞清楚几个知识点： Socket、TCP 详解 IO多路复用（select、epoll、kqueue） ioloop.IOLoop，继承至 tornado.util.Configurable（主要用于子类的创建）。IOLoop 首先会获取一个全局锁，以保证全局只有一个 IOLoop，即单例模式。IOLoop 使用事例： import errno import functools import tornado.ioloop import socket def connection_ready(sock, fd, events): while True: try: connection, address = sock.accept() except socket.error as e: if e.args[0] not in (errno.EWOULDBLOCK, errno.EAGAIN): raise return connection.setblocking(0) handle_connection(connection, address) if __name__ == '__main__': sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0) sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) sock.setblocking(0) sock.bind((\"\", port)) sock.listen(128) # 获取IOLoop对象 io_loop = tornado.ioloop.IOLoop.current() callback = functools.partial(connection_ready, sock) io_loop.add_handler(sock.fileno(), callback, io_loop.READ) # 启动IOLoop io_loop.start() 从事例中知道，使用 ioloop 事件循环，首先需要获取 IOLoop 对象，即调用 tornado.ioloop.IOLoop.current() ","date":"2017-08-25","objectID":"/posts/tornado_ioloop/:1:0","tags":["Tornado"],"title":"tornado ioloop 实现解析","uri":"/posts/tornado_ioloop/"},{"categories":["Tornado 源码解析"],"content":"tornado.ioloop.IOLoop.current() @staticmethod def current(instance=True): current = getattr(IOLoop._current, \"instance\", None) if current is None and instance: return IOLoop.instance() return current 该函数只有四行，它先判断当前线程中是否有 IOLoop 实例正在运行或者被 IOLoop.make_current() 标记过，如果结果为真就直接返回当前 IOLoop，否则调用 IOLoop.instance() 去创建 IOLoop 实例。 ","date":"2017-08-25","objectID":"/posts/tornado_ioloop/:1:1","tags":["Tornado"],"title":"tornado ioloop 实现解析","uri":"/posts/tornado_ioloop/"},{"categories":["Tornado 源码解析"],"content":"tornado.ioloop.IOLoop.instance() @staticmethod def instance(): # 第一层检查 if not hasattr(IOLoop, \"_instance\"): with IOLoop._instance_lock: # 第二层检查 if not hasattr(IOLoop, \"_instance\"): # New instance after double check IOLoop._instance = IOLoop() return IOLoop._instance 在该方法中使用了两次判断，用以实现单例。第一层判断只是为了提升性能，其实只需第二层判断就已经可以实现单例模式。但是如果没有第一层判断，我们只有实例化 IOLoop 的时候需要加锁，其他时候 IOLoop 实例已经存在了，不需要加锁了，但是这时其他需要 IOLoop 实例的线程还是必须等待锁，锁的存在明显降低了效率，有性能损耗。 两层检查都没通过时，会初始化 IOLoop 对象（调用 IOLoop.instance = IOLoop()）。此时，调用 IOLoop 父类 tornado.util.Configurable 的 __new_() 方法实例化 IOLoop 对象。因为没有调用 tornado.util.Configurable.configure() 函数配置实现类，因此 Configurable 会调用 tornado.ioloop.IOLoop.configurable_default() 默认配置实现类（不同系统选择不同的 IO 多路复用机制，即epoll、kqueue、select）。 ","date":"2017-08-25","objectID":"/posts/tornado_ioloop/:1:2","tags":["Tornado"],"title":"tornado ioloop 实现解析","uri":"/posts/tornado_ioloop/"},{"categories":["Tornado 源码解析"],"content":"tornado.ioloop.IOLoop.configurable_default() @classmethod def configurable_default(cls): # 判断系统是否实现epoll，如果有就使用EPollIOLoop作为实现类 if hasattr(select, \"epoll\"): from tornado.platform.epoll import EPollIOLoop return EPollIOLoop # 判断系统是否实现kqueue， 如果有就使用KQueueIOLoop作为实现类 if hasattr(select, \"kqueue\"): # Python 2.6+ on BSD or Mac from tornado.platform.kqueue import KQueueIOLoop return KQueueIOLoop # 否则使用SelectIOLoop作为实现类 from tornado.platform.select import SelectIOLoop return SelectIOLoop 此处我们默认选择 EPollIOLoop 作为实现类。通过 Configurable 会生成 EPollIOLoop 实例，并调用 tornado.platform.epoll.EPollIOLoop.initialize()。 ","date":"2017-08-25","objectID":"/posts/tornado_ioloop/:1:3","tags":["Tornado"],"title":"tornado ioloop 实现解析","uri":"/posts/tornado_ioloop/"},{"categories":["Tornado 源码解析"],"content":"tornado.platform.epoll.EPollIOLoop class EPollIOLoop(PollIOLoop): def initialize(self, **kwargs): # 调用PollIOLoop.initialize() super(EPollIOLoop, self).initialize(impl=select.epoll(), **kwargs) ","date":"2017-08-25","objectID":"/posts/tornado_ioloop/:1:4","tags":["Tornado"],"title":"tornado ioloop 实现解析","uri":"/posts/tornado_ioloop/"},{"categories":["Tornado 源码解析"],"content":"tornado.ioloop.PollIOLoop.initialize() def initialize(self, impl, time_func=None, **kwargs): super(PollIOLoop, self).initialize(**kwargs) self._impl = impl if hasattr(self._impl, 'fileno'): # 设置select.epoll描述符在子进程执行exec()族函数时自动关掉 set_close_exec(self._impl.fileno()) self.time_func = time_func or time.time self._handlers = {} self._events = {} self._callbacks = collections.deque() self._timeouts = [] self._cancellations = 0 self._running = False self._stopped = False self._closing = False self._thread_ident = None self._blocking_signal_threshold = None self._timeout_counter = itertools.count() # 创建一个pipe（管道），当IOLoop处于空闲，即无读写事件时， # 会向该pipe中发送虚假数据从而叫醒IOLoop self._waker = Waker() # 将唤醒者（waker）读文件描述符注册到epoll， # 当另一线程调用tornado.ioloop.PollIOLoop.add_callback()时， # 会调用self._waker.wake()唤醒IOLoop，从而让注册的回调函数运行。 # 由于唤醒正在轮训中的IOLoop比它自动唤醒（超时）消耗资源相对昂贵， # 所以在IOLoop同一线程中不会去主动唤醒它。 self.add_handler(self._waker.fileno(), lambda fd, events: self._waker.consume(), self.READ) 方法中都是进行一些全局数据的初始化工作。其中对 epoll 文件描述符设置 fork 时自动关闭以及叫醒 IOLoop 机制可参考 tornado_platform_posix.md IOLoop 初始化完成之后就会调用 IOLoop.start() 方法去启动 IOLoop，是 IOLoop 的核心。此方法在 PollIOLoop 中实现。 ","date":"2017-08-25","objectID":"/posts/tornado_ioloop/:1:5","tags":["Tornado"],"title":"tornado ioloop 实现解析","uri":"/posts/tornado_ioloop/"},{"categories":["Tornado 源码解析"],"content":"tornado.ioloop.PollIOLoop.start() def start(self): if self._running: raise RuntimeError(\"IOLoop is already running\") self._setup_logging() if self._stopped: self._stopped = False return old_current = getattr(IOLoop._current, \"instance\", None) IOLoop._current.instance = self self._thread_ident = thread.get_ident() self._running = True # signal.set_wakeup_fd解决了事件循环中的条件竞争：select/poll/etc在开始进入 # 中断休眠之前一个信号可能会到达，因此信号可能在没有唤醒select时被处理消耗掉。 # 解决方法与C语言中的同步机制一样，将信号处理程序写入管道，然后通过select获取。 old_wakeup_fd = None if hasattr(signal, 'set_wakeup_fd') and os.name == 'posix': try: # 将唤醒者写管道文件描述符注册为wakeup_fd，当一个信号到来 # 时（如SIGINT，即Ctrl+C），会向其中写入\"\\0\"，从而唤醒select或poll。 # 该函数返回上次设置的文件描述符，如果之前没设置过，则返回-1 old_wakeup_fd = signal.set_wakeup_fd(self._waker.write_fileno()) # 当返回值为-1则说明之前已经设置过wakeup_fd，然后重置 if old_wakeup_fd != -1: signal.set_wakeup_fd(old_wakeup_fd) old_wakeup_fd = None except ValueError: old_wakeup_fd = None try: while True: # 为了避免IO事件饥饿，将新添加的回调延迟到事件循环的下一次迭代中。 # epoll水平触发模式下，当有大量文件描述符就绪需要处理时， # 可能会导致事件太多而没有执行到新添加的回调，这样就会造成IO事件饥饿。 ncallbacks = len(self._callbacks) # timeouts是tornado封装好的超时处理器 due_timeouts = [] # 保存本次迭代需要执行的超时任务 if self._timeouts: now = self.time() while self._timeouts: # 超时事件的回调函数已经被取消，即该超时事件已无效 if self._timeouts[0].callback is None: heapq.heappop(self._timeouts) self._cancellations -= 1 # 当前时间是否已超过超时事件的最后期限，如已超过，则将其取出并 # 保存到due_timeouts，然后执行 elif self._timeouts[0].deadline \u003c= now: due_timeouts.append(heapq.heappop(self._timeouts)) else: break # 优化，当超时事件被取消的次数大于512次并且大于超时事件数量的一 # 半时，清理所有被取消的超时事件 if (self._cancellations \u003e 512 and self._cancellations \u003e (len(self._timeouts) \u003e\u003e 1)): self._cancellations = 0 self._timeouts = [x for x in self._timeouts if x.callback is not None] heapq.heapify(self._timeouts) # 执行所有callback函数，及已经超时的超时事件 for i in range(ncallbacks): self._run_callback(self._callbacks.popleft()) for timeout in due_timeouts: if timeout.callback is not None: self._run_callback(timeout.callback) # 释放资源 due_timeouts = timeout = None # 如果有回调函数，则epoll.poll(timeout)函数的timeout为0 if self._callbacks: poll_timeout = 0.0 # 如果没有回调函数且有超时事件，则poll的timeout为： # 首先获取最近的超时事件的最后期限与当前事件的差值，然后该值与poll的 # timeout的默认值两者取较小值，再与0比较取较大值 elif self._timeouts: poll_timeout = self._timeouts[0].deadline - self.time() poll_timeout = max(0, min(poll_timeout, _POLL_TIMEOUT)) else: poll_timeout = _POLL_TIMEOUT if not self._running: break # 取消信号定时器 if self._blocking_signal_threshold is not None: signal.setitimer(signal.ITIMER_REAL, 0, 0) try: # 整个方法的核心，即epoll.poll(timeout) event_pairs = self._impl.poll(poll_timeout) except Exception as e: if errno_from_exception(e) == errno.EINTR: continue else: raise if self._blocking_signal_threshold is not None: signal.setitimer(signal.ITIMER_REAL, self._blocking_signal_threshold, 0) # 处理IO事件 self._events.update(event_pairs) while self._events: fd, events = self._events.popitem() try: # 通过文件描述符获取在PollIOLoop.add_handler() # 方法中绑定到self._handlers中的socket对象及处理函数 fd_obj, handler_func = self._handlers[fd] # 调用处理器函数 handler_func(fd_obj, events) except (OSError, IOError) as e: if errno_from_exception(e) == errno.EPIPE: pass else: # 如果有异常则调用异常处理函数 self.handle_callback_exception(self._handlers.get(fd)) except Exception: self.handle_callback_exception(self._handlers.get(fd)) # 释放资源 fd_obj = handler_func = None finally: self._stopped = False if self._blocking_signal_threshold is not None: signal.setitimer(signal.ITIMER_REAL, 0, 0) IOLoop._current.instance = old_current if old_wakeup_fd is not None: signal.set_wakeup_fd(old_wakeup_fd) 对 start() 方法中的 timeouts 的详解可以参考 tornado_ioloop_PeriodicCallback.md，其中针对特定实例做了相关分析，能加深理解。callbacks 与 timeouts 的处理是相似的。 tornado IOLoop 中对 epoll.register()、epoll.modify()、epoll.unregister() 分别做了封装，对应 PollIOLoop.add_handler()、PollIOLoop.update_handler()、PollIOLoop.remove_handler()，分别表示注册文件描述符到 epoll 中、更新 epoll 中监听文件描述符的事件类型、删除 epoll 中监听的文件描述符。 ","date":"2017-08-25","objectID":"/posts/tornado_ioloop/:1:6","tags":["Tornado"],"title":"tornado ioloop 实现解析","uri":"/posts/tornado_ioloop/"},{"categories":["Tornado 源码解析"],"content":"IO 多路复用（Reactor） IO 多路复用技术是为实现单线程处理多请求连接，减少系统因频繁的创建线程或进程而产生的资源消耗，这里的复用特指同时使用单一线程。 Linux 下的 select、poll、epoll 为 IO 多路复用的具体实现。当客户端与服务端的 socket 连接建立之后，程序将该 socket 文件描述符注册到 epoll，然后返回，最终交由 epoll 去管理。 epoll 可以同时监听多个文件描述符，当某个或某些文件描述符就绪，则通知程序进行相应的读写操作，否则会一直阻塞直到有文件描述符就绪。我们使用epoll编程时，会设置 socket 非阻塞模式。所以，IO多路复用是同步非阻塞 IO。 ","date":"2017-08-24","objectID":"/posts/io_multiplexing/:1:0","tags":["Tornado","Reactor"],"title":"IO 多路复用（Reactor）","uri":"/posts/io_multiplexing/"},{"categories":["Tornado 源码解析"],"content":"select、poll、epoll 比较 ","date":"2017-08-24","objectID":"/posts/io_multiplexing/:2:0","tags":["Tornado","Reactor"],"title":"IO 多路复用（Reactor）","uri":"/posts/io_multiplexing/"},{"categories":["Tornado 源码解析"],"content":"select 优点 select 目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。 缺点 每次调用 select 都需要把文件描述符（FD）从用户态拷贝到内核，开销比较大 每次都需要在内核遍历所有传入的文件描述符（FD） select 单个进程能够监视的文件描述符的数量存在最大限制，默认是1024，比较小。当然，也可以通过修改宏定义改掉，但这会造成效率的降低。 ","date":"2017-08-24","objectID":"/posts/io_multiplexing/:2:1","tags":["Tornado","Reactor"],"title":"IO 多路复用（Reactor）","uri":"/posts/io_multiplexing/"},{"categories":["Tornado 源码解析"],"content":"poll poll 即轮训，poll 和 select 本质上是一样的，只是描述 fd 集合的方式不同。poll 使用的是 pollfd 结构，select 使用的是 fd_set 结构。 ","date":"2017-08-24","objectID":"/posts/io_multiplexing/:2:2","tags":["Tornado","Reactor"],"title":"IO 多路复用（Reactor）","uri":"/posts/io_multiplexing/"},{"categories":["Tornado 源码解析"],"content":"epoll epoll 是对 select 和 poll 的改进，而且改正了 select、poll 的三个缺点和不足。相对于 select 和 poll 来说，epoll 更加灵活，没有描述符限制。 epoll 使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的 copy 只需一次。 优点 每次注册新事件到 epoll 句柄都会把所有的 fd 拷贝进来，而不是在 epoll_wait 中重复拷贝，这样确保 fd 只会被拷贝一次 epoll 不是像 select/poll 那样每次都把 fd 加入等待队列，epoll 把每个 fd 指定一个回调函数，当设备就绪时，唤醒等待队列的等待者就会调用其它的回调函数，这个回调函数会把就绪的 fd 放入一个就绪链表。epoll_wait 就是在这个就绪链表中查看有没有就绪 fd。 epoll 没有 fd 数目限制 缺点 如果没有大量的 idle-connection 或者 dead-connection，epoll 的效率并不会比 select/poll 高很多，但是当遇到大量的 idle-connection，就会发现 epoll 的效率大大高于 select/poll。 模式 水平触发（level-triggered）：满足状态时触发 当被监控的文件描述符上有可读写事件发生时，epoll_wait() 会通知处理程序去读写。如果这次没有把数据一次性全部读写完(如读写缓冲区太小)，那么下次调用 epoll_wait() 时，它还会通知你在上次没读写完的文件描述符上继续读写，当然如果你一直不去读写，它会一直通知你！！！如果系统中有大量你不需要读写的就绪文件描述符，而它们每次都会返回，这样会大大降低处理程序检索自己关心的就绪文件描述符的效率！！！ 边缘触发（edge-triggered）：状态改变时触发 当被监控的文件描述符上有可读写事件发生时，epoll_wait() 会通知处理程序去读写。如果这次没有把数据全部读写完(如读写缓冲区太小)，那么下次调用 epoll_wait() 时，它不会通知你，也就是它只会通知你一次，直到该文件描述符上出现第二次可读写事件才会通知你去读写余下的数据！！！这种模式比水平触发效率高，系统不会充斥大量你不关心的就绪文件描述符！！！ 例如一个 socket 经过长时间等待后接收到一段 100k 的数据，两种触发方式都会向程序发出就绪通知。假设程序从这个socket 中读取了 50k 数据，并再次调用监听函数，水平触发依然会发出就绪通知，而边缘触发会因为 socket “有数据可读”这个状态没有发生变化而不发出通知且陷入长时间的等待。 ","date":"2017-08-24","objectID":"/posts/io_multiplexing/:2:3","tags":["Tornado","Reactor"],"title":"IO 多路复用（Reactor）","uri":"/posts/io_multiplexing/"},{"categories":["Tornado 源码解析"],"content":"总结 select，poll 实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用 epoll_wait 不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪 fd 放入就绪链表中，并唤醒在 epoll_wait 中进入睡眠的进程。虽然都要睡眠和交替，但是 select 和 poll 在“醒着”的时候要遍历整个 fd 集合，而 epoll 在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的 CPU 时间。这就是回调机制带来的性能提升。 select，poll 每次调用都要把 fd 集合从用户态往内核态拷贝一次，并且要把 current 往设备等待队列中挂一次，而epoll 只要一次拷贝，而且把 current 往等待队列上挂也只挂一次（在 epoll_wait 的开始，注意这里的等待队列并不是设备等待队列，只是一个 epoll 内部定义的等待队列）。这也能节省不少的开销。 ","date":"2017-08-24","objectID":"/posts/io_multiplexing/:3:0","tags":["Tornado","Reactor"],"title":"IO 多路复用（Reactor）","uri":"/posts/io_multiplexing/"},{"categories":["Tornado 源码解析"],"content":"Socket socket 是网络进程之间的通讯方式，是对 TCP/IP 协议的封装。socket 并不是像 HTTP、TCP、IP 一样的协议，而是一组调用接口（API）。通过 socket，可以使用 TCP/IP 协议，即可以在网络上传输数据。 http 是应用层协议，web 开发中最常见的协议。当我们在浏览器输入一个网址，比如 http://www.google.com 时: 浏览器首先会去查看本地 hosts 文件，通过域名（google.com）获取其对应的 IP 地址，如果本地没有找到，则继续向上层请求 DNS 服务器，DNS 服务器就像一个树结构，一层一层的向上递进查找。 获取 IP 地址之后，浏览器会根据 IP 与默认端口 80，通过 TCP 协议三次握手与服务器建立 socket 连接。 在 Linux 下，一切皆文件。系统将每一个 socket 连接也抽象成一个文件，客户端与服务器建立连接之后，各自在本地进程中维护一个文件，然后可以向自己文件写入内容供对方读取或者读取对方内容，通讯结束时关闭文件。 ","date":"2017-08-24","objectID":"/posts/socket_tcp/:1:0","tags":["Tornado","Socket","TCP"],"title":"Socket、TCP 详解","uri":"/posts/socket_tcp/"},{"categories":["Tornado 源码解析"],"content":"TCP 三次握手、四次挥手 TCP 连接示意图 ","date":"2017-08-24","objectID":"/posts/socket_tcp/:2:0","tags":["Tornado","Socket","TCP"],"title":"Socket、TCP 详解","uri":"/posts/socket_tcp/"},{"categories":["Tornado 源码解析"],"content":"三次握手 第一次握手 客户端尝试连接服务器，向服务器发送 syn 包（同步序列编号 Synchronize Sequence Numbers），syn=j，客户端进入 SYN_SEND 状态等待服务器确认； 第二次握手 服务器接收客户端 syn 包并确认（ack=j+1），同时向客户端发送一个 SYN 包（syn=k），即 SYN+ACK 包，此时服务器进入 SYN_RECV 状态；此时，内核将连接（socket）放入 SYN QUEUE，即 未完成队列 ，该队列大小由 /proc/sys/net/ipv4/tcp_max_syn_backlog 设定； 第三次握手 客户端收到服务器的 SYN+ACK 包，向服务器发送确认包 ACK(ack=k+1），此包发送完毕，客户端和服务器进入 ESTABLISHED状态，完成三次握手。此时，内核将连接（socket）移到 ACCEPT QUEUE，即 已完成队列 ，队列大小为 socket listen(backlog) 函数传入的 backlog 参数与 /proc/sys/net/core/somaxconn 决定，取二者最小值。服务器程序调用 accept 函数后，该连接（socket）被内核从已完成队列移除，并交由服务器程序控制。 注意 : TCP 三次握手在应用程序调用 accept 函数之前由内核完成。调用 accept 只是获取已经完成的连接。 ","date":"2017-08-24","objectID":"/posts/socket_tcp/:2:1","tags":["Tornado","Socket","TCP"],"title":"Socket、TCP 详解","uri":"/posts/socket_tcp/"},{"categories":["Tornado 源码解析"],"content":"四次挥手 第一次挥手 主机1（可以是客户端，也可以是服务器端），设置 Sequence Number 和 Acknowledgment Number，向主机2发送一个 FIN 报文段；此时，主机1进入 FIN_WAIT_1 状态；这表示主机1没有数据要发送给主机2了； 第二次挥手 主机2收到了主机1发送的 FIN 报文段，向主机1回一个 ACK 报文段，Acknowledgment Number 为 Sequence Number 加1；主机1进入 FIN_WAIT_2 状态；主机2告诉主机1，我“同意”你的关闭请求； 第三次挥手 主机2向主机1发送FIN报文段，请求关闭连接，同时主机2进入 LAST_ACK 状态； 第四次挥手 主机1收到主机2发送的 FIN 报文段，向主机2发送 ACK 报文段，然后主机1进入 TIME_WAIT 状态；主机2收到主机1的 ACK 报文段以后，就关闭连接；此时，主机1等待 2MSL 后依然没有收到回复，则证明 Server 端已正常关闭，那么，主机1也可以关闭连接了 ","date":"2017-08-24","objectID":"/posts/socket_tcp/:2:2","tags":["Tornado","Socket","TCP"],"title":"Socket、TCP 详解","uri":"/posts/socket_tcp/"},{"categories":["Tornado 源码解析"],"content":"客户端状态转换过程 客户端状态 ","date":"2017-08-24","objectID":"/posts/socket_tcp/:2:3","tags":["Tornado","Socket","TCP"],"title":"Socket、TCP 详解","uri":"/posts/socket_tcp/"},{"categories":["Tornado 源码解析"],"content":"服务端状态转换过程 服务端状态 ","date":"2017-08-24","objectID":"/posts/socket_tcp/:2:4","tags":["Tornado","Socket","TCP"],"title":"Socket、TCP 详解","uri":"/posts/socket_tcp/"},{"categories":["Tornado 源码解析"],"content":"Socket 编程示例代码 ","date":"2017-08-24","objectID":"/posts/socket_tcp/:3:0","tags":["Tornado","Socket","TCP"],"title":"Socket、TCP 详解","uri":"/posts/socket_tcp/"},{"categories":["Tornado 源码解析"],"content":"server.py import socket import StringIO HOST = \"127.0.0.1\" PORT = 3267 # 创建一个IPV4且基于TCP协议的socket对象 server = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # 绑定监听端口 server.bind((HOST, PORT)) # 开始监听端口，并且等待连接的最大数量为5 server.listen(5) print \"waiting for connection...\" while True: # 接受连接（此方法阻塞） conn, addr = server.accept() print \"Connected by \", addr buffer = StringIO.StringIO() while True: # 每次最多读取1k数据 data = conn.recv(1024) if data: print \"receive client data: \", data buffer.write(data) conn.sendall(\"Hello, {}\".format(data)) else: break print \"receive client ALL datas: \", buffer.getvalue() buffer.close() conn.close() print 'Connection from %s:%s closed.' % addr ","date":"2017-08-24","objectID":"/posts/socket_tcp/:3:1","tags":["Tornado","Socket","TCP"],"title":"Socket、TCP 详解","uri":"/posts/socket_tcp/"},{"categories":["Tornado 源码解析"],"content":"client.py import socket HOST = \"127.0.0.1\" PORT = 3267 # 创建一个IPV4且基于TCP协议的socket对象 client = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # 建立连接 client.connect((HOST, PORT)) while True: try: # 获取用户控制台输入 data = raw_input(\"please input something: \") except: # 关闭客户端连接 client.close() break client.send(data) result = client.recv(1024) print \"client result: \", result ","date":"2017-08-24","objectID":"/posts/socket_tcp/:3:2","tags":["Tornado","Socket","TCP"],"title":"Socket、TCP 详解","uri":"/posts/socket_tcp/"},{"categories":["Tornado 源码解析"],"content":"运行过程 运行 server.py 文件，系统进程会启动一个本地 socket，该 socket 会一直循环等待客户端 socket 的连接。 当我们运行 client.py 文件后，客户端会与服务端建立连接。此时，客户端文件会读取用户在控制台的输入数据，然后发送给服务器。 ","date":"2017-08-24","objectID":"/posts/socket_tcp/:3:3","tags":["Tornado","Socket","TCP"],"title":"Socket、TCP 详解","uri":"/posts/socket_tcp/"},{"categories":["Tornado 源码解析"],"content":"问题 当多开一个控制台再次运行 client.py 时，此时是无法与服务器建立连接的。因为该程序是单线程的，也就是同一时间服务器只能被一个客户端连接。 ","date":"2017-08-24","objectID":"/posts/socket_tcp/:3:4","tags":["Tornado","Socket","TCP"],"title":"Socket、TCP 详解","uri":"/posts/socket_tcp/"},{"categories":["Tornado 源码解析"],"content":"传统解决方法 传统解决方式就是多线程，一个客户端连接开启一个线程去处理。修改后的 server.py 如下： import socket import StringIO import threading HOST = \"127.0.0.1\" PORT = 3267 # 创建一个IPV4且基于TCP协议的socket对象 server = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # 绑定监听端口 server.bind((HOST, PORT)) # 开始监听端口，并且等待连接的最大数量为5 server.listen(5) print \"waiting for connection...\" def handle_conn(conn, addr): buffer = StringIO.StringIO() while True: # 每次最多读取1k数据 data = conn.recv(1024) if data: print \"receive client data: \", data buffer.write(data) conn.sendall(\"Hello, {}\".format(data)) else: break print \"receive client ALL datas: \", buffer.getvalue() buffer.close() conn.close() print 'Connection from %s:%s closed.' % addr while True: # 接受连接（此方法阻塞） conn, addr = server.accept() print \"Connected by \", addr threading.Thread(target=handle_conn, args=(conn, addr)).start() ","date":"2017-08-24","objectID":"/posts/socket_tcp/:3:5","tags":["Tornado","Socket","TCP"],"title":"Socket、TCP 详解","uri":"/posts/socket_tcp/"},{"categories":["Tornado 源码解析"],"content":"优化解决方法 传统解决方法显然不现实，无法承受高并发、高访问量，线程对资源的消耗太大。为解决此问题，引入新概念：IO多路复用（Linux 下 select/poll/epoll），即事件驱动，所谓的 Reactor 模式。它实现了单线程连接多客户端。使用 epoll 实现 server.py： import socket import StringIO import select HOST = \"127.0.0.1\" PORT = 3267 # 创建一个IPV4且基于TCP协议的socket对象 server = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # 绑定监听端口 server.bind((HOST, PORT)) # 开始监听端口，并且等待连接的最大数量为5 server.listen(5) # 设置非阻塞 server.setblocking(0) # 获取epoll对象 epoll = select.epoll() # 向epoll中注册服务器socket描述符，监听读事件 epoll.register(server.fileno(), select.EPOLLIN) print \"waiting for connection...\" connections = {} requests = {} responses = {} while True: # 当监听的socket文件描述符发生改变则会以列表的形式主动报告给用户进程（阻塞） # timeout为获取结果的时间（秒） 当timeout等于-1的时永远等待(默认就是-1)直 # 到文件描述符发生改变，如果指定为1那么epoll每1秒进行汇报一次当前文件描述符 # 的状态哪怕是没有文件描述符发生改变也会返回一个空值 events = epoll.poll(-1) # 循环发生改变的socket，返回的每个对象都是一个元组：(socket描述符, 监听的事件) for fileno, event in events: # 判断是否为服务器socket描述符，如果是则为第一个连接 if fileno == server.fileno(): # 接受连接（此方法阻塞），每一次与客户端建立连接都会创建一个socket，即下面的conn conn, addr = server.accept() print \"Connected by \", addr conn_fd = conn.fileno() # 设置socket连接为非阻塞 conn.setblocking(0) # 将socket连接注册到epoll，监听读事件 epoll.register(conn_fd, select.EPOLLIN) # 保存所有连接 connections[conn_fd] = (conn, addr) # 判断是否为断开事件 elif event \u0026 select.EPOLLHUP: # 解除epoll对socket的监听 epoll.unregister(fileno) # 关闭socket连接 connections[fileno][0].close() del connections[fileno] print 'Connection from %s:%s closed.' % connections[fileno][1] # 判断是否为读事件 elif event \u0026 select.EPOLLIN: conn = connections[fileno][0] buffer = StringIO.StringIO() while True: try: # 每次最多读取1k数据 data = conn.recv(1024) except socket.error, e: # import traceback # print traceback.format_exc() break if data: print \"receive client data: \", data buffer.write(data) conn.sendall(\"Hello, {}\".format(data)) else: break print \"receive client ALL datas: \", buffer.getvalue() requests[fileno] = buffer.getvalue().strip() epoll.modify(fileno, select.EPOLLOUT) buffer.close() # 判断是否为写事件 elif event \u0026 select.EPOLLOUT: connections[fileno].send(requests[fileno]) # 写事件完成之后将该socket从监听写事件改为监听读事件 epoll.modify(fileno, select.EPOLLIN) ","date":"2017-08-24","objectID":"/posts/socket_tcp/:3:6","tags":["Tornado","Socket","TCP"],"title":"Socket、TCP 详解","uri":"/posts/socket_tcp/"},{"categories":["Tornado 源码解析"],"content":"tornado 源码解析 ","date":"2017-08-24","objectID":"/posts/readme/:1:0","tags":["Tornado"],"title":"tornado 源码解析","uri":"/posts/readme/"},{"categories":["Tornado 源码解析"],"content":"简介 此项目主要是针对 python web 框架 — tornado 源码相关模块进行解析，加深对 web 开发的理解。在详解某一模块时会引入相关基础知识概念。包括以下几个知识点： Socket、TCP 详解 IO 多路复用（select、epoll、kqueue） tornado.ioloop 实现解析 tornado posix 实现解析 tornado 配置类 Configurable 实现解析 tornado.httpserver 实现解析 tornado netutil 实现解析 tornado.http1connection 实现解析 tornado.iostream 实现解析 tornado.gen 实现解析 tornado 定时器实现解析 tornado.concurrent 实现解析 tornado 多进程实现解析 ","date":"2017-08-24","objectID":"/posts/readme/:1:1","tags":["Tornado"],"title":"tornado 源码解析","uri":"/posts/readme/"},{"categories":["Tornado 源码解析"],"content":"环境 python 2.7 tornado 4.5.1 Ubuntu 16.04 ","date":"2017-08-24","objectID":"/posts/readme/:1:2","tags":["Tornado"],"title":"tornado 源码解析","uri":"/posts/readme/"}]